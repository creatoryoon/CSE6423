{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c29f2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "[-1.2  -0.07]\n",
      "[0.6  0.07]\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "#https://davinci-ai.tistory.com/33\n",
    "\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "env = gym.make('MountainCar-v0')   # env = 언덕, 왼쪽과 오른쪽을 반복하여 가속도를 만들어야 오를 수 있음.\n",
    "print(env.observation_space) # 관찰공간, agent가 환경을 볼 수 있는 범위\n",
    "print(env.observation_space.low)\n",
    "print(env.observation_space.high)\n",
    "\n",
    "print(env._max_episode_steps) # 각 ep마다의 종료 조건, 해당값이 n이면 최대 n의 timestep를 가짐, n번 움직이면 종료\n",
    "# agent : car\n",
    "# action : left, stop, right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "260cc4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space) # agent가 할 수 있는 행동의 경우의 수, discrete: 이산적, 정수로 나눌수 있음\n",
    "# reward: 200이하의 step에서 time step마다 -1, 깃발(x축 0.5지점)에 도착하는 것(종료 조건, 최대 보상 조건)이 목표임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6641166",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0') # 환경 생성\n",
    "\n",
    "env.reset() # 환경 초기화\n",
    "step = 0\n",
    "score = 0\n",
    "env.render() # 실행결과 화면으로 출력\n",
    "env.close()\n",
    "## 여기까지가 기본적인 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e08fb018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!\n",
      "mean of scores -199.91\n",
      "length of acceted_scores 2\n",
      "mean of acceted_scores -191.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0') \n",
    "env.reset() # 환경 초기화\n",
    "scores = []\n",
    "training_data = [] \n",
    "accepted_scores = [] \n",
    "required_score = -198 \n",
    "\n",
    "\n",
    "for i in range(200): \n",
    "    env.reset() \n",
    "    score = 0 \n",
    "    game_memory = [] \n",
    "    previous_obs = [] \n",
    "    \n",
    "    while True: \n",
    "        action = env.action_space.sample() \n",
    "        obs, reward, done, info = env.step(action) \n",
    "        if len(previous_obs) > 0: \n",
    "            game_memory.append([previous_obs, action]) \n",
    "        previous_obs = obs \n",
    "        if obs[0] > -0.2: \n",
    "            reward = 1 \n",
    "        score += reward \n",
    "        if done: \n",
    "            break \n",
    "    scores.append(score) \n",
    "    if score > required_score: \n",
    "        accepted_scores.append(score) \n",
    "        for data in game_memory: \n",
    "            training_data.append(data) \n",
    "\n",
    "print('finished!') \n",
    "print('mean of scores', np.mean(scores)) \n",
    "print('length of acceted_scores', len(accepted_scores)) \n",
    "print('mean of acceted_scores', np.mean(accepted_scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5edd34c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([-4.17207523e-01,  2.17090725e-04]), 1], [array([-0.41777489, -0.00056736]), 2], [array([-4.18122666e-01, -3.47778596e-04]), 0], [array([-0.42024838, -0.00212571]), 1], [array([-0.42313686, -0.00288848]), 0], [array([-0.42776745, -0.00463059]), 2], [array([-0.43210692, -0.00433947]), 1], [array([-0.43712399, -0.00501707]), 2], [array([-0.44178238, -0.00465838]), 1], [array([-0.44704824, -0.00526586]), 2], [array([-0.4518832 , -0.00483496]), 2], [array([-0.45625188, -0.00436869]), 2], [array([-0.46012224, -0.00387036]), 0], [array([-0.4654658 , -0.00534356]), 2], [array([-0.47024315, -0.00477735]), 0], [array([-0.47641896, -0.00617581]), 0], [array([-0.48394744, -0.00752848]), 0], [array([-0.49277261, -0.00882516]), 0], [array([-0.50282864, -0.01005603]), 0], [array([-0.51404034, -0.01121171]), 1], [array([-0.52532372, -0.01128338]), 2], [array([-0.53559417, -0.01027045]), 2], [array([-0.54477467, -0.0091805 ]), 2], [array([-0.55279646, -0.00802179]), 1], [array([-0.56059955, -0.00780309]), 1], [array([-0.5681257 , -0.00752615]), 0], [array([-0.57631889, -0.00819319]), 1], [array([-0.58411832, -0.00779944]), 0], [array([-0.59246635, -0.00834803]), 0], [array([-0.60130154, -0.00883519]), 0], [array([-0.61055922, -0.00925768]), 1], [array([-0.61917207, -0.00861285]), 0], [array([-0.6280779 , -0.00890583]), 1], [array([-0.63621291, -0.00813501]), 1], [array([-0.64351928, -0.00730638]), 0], [array([-0.65094553, -0.00742625]), 0], [array([-0.65843974, -0.00749421]), 2], [array([-0.66395   , -0.00551026]), 0], [array([-0.66943846, -0.00548846]), 0], [array([-0.67486768, -0.00542922]), 0], [array([-0.68020093, -0.00533325]), 2], [array([-0.68340239, -0.00320146]), 2], [array([-0.68445071, -0.00104832]), 2], [array([-0.68333892,  0.00111179]), 0], [array([-0.68207441,  0.00126451]), 0], [array([-0.6806656,  0.0014088]), 2], [array([-0.67712191,  0.0035437 ]), 2], [array([-0.67146707,  0.00565484]), 2], [array([-0.66373923,  0.00772784]), 0], [array([-0.65599103,  0.0077482 ]), 2], [array([-0.64627579,  0.00971524]), 1], [array([-0.6356611 ,  0.01061469]), 0], [array([-0.62522169,  0.01043941]), 0], [array([-0.61503186,  0.01018983]), 1], [array([-0.60416484,  0.01086701]), 2], [array([-0.59169945,  0.01246539]), 0], [array([-0.57972685,  0.0119726 ]), 1], [array([-0.56733527,  0.01239158]), 1], [array([-0.55461661,  0.01271866]), 1], [array([-0.54166565,  0.01295096]), 1], [array([-0.52857926,  0.01308639]), 1], [array([-0.51545551,  0.01312375]), 2], [array([-0.50139283,  0.01406268]), 2], [array([-0.48649658,  0.01489626]), 0], [array([-0.47287801,  0.01361856]), 0], [array([-0.46063839,  0.01223962]), 1], [array([-0.44886816,  0.01177022]), 1], [array([-0.43765374,  0.01121443]), 2], [array([-0.42607678,  0.01157696]), 1], [array([-0.41522084,  0.01085593]), 0], [array([-0.4061635 ,  0.00905734]), 0], [array([-0.39896881,  0.00719469]), 1], [array([-0.39268723,  0.00628159]), 0], [array([-0.38836243,  0.0043248 ]), 1], [array([-0.38502431,  0.00333812]), 1], [array([-0.38269581,  0.00232849]), 1], [array([-0.3813929 ,  0.00130291]), 0], [array([-0.38212448, -0.00073157]), 1], [array([-0.38388554, -0.00176106]), 2], [array([-0.38566403, -0.0017785 ]), 2], [array([-0.38744777, -0.00178374]), 1], [array([-0.39022448, -0.00277671]), 0], [array([-0.39497502, -0.00475054]), 1], [array([-0.40066648, -0.00569146]), 0], [array([-0.40825917, -0.00759269]), 0], [array([-0.41769975, -0.00944058]), 2], [array([-0.42692128, -0.00922153]), 0], [array([-0.43785776, -0.01093649]), 2], [array([-0.44843024, -0.01057248]), 0], [array([-0.46056171, -0.01213147]), 0], [array([-0.47416315, -0.01360144]), 0], [array([-0.489134  , -0.01497085]), 1], [array([-0.50436287, -0.01522887]), 0], [array([-0.52073594, -0.01637306]), 1], [array([-0.53713047, -0.01639453]), 1], [array([-0.55342355, -0.01629307]), 0], [array([-0.57049323, -0.01706969]), 1], [array([-0.58721237, -0.01671914]), 0], [array([-0.6044573 , -0.01724493]), 0], [array([-0.62210171, -0.01764442]), 0], [array([-0.64001807, -0.01791635]), 0], [array([-0.65807891, -0.01806085]), 2], [array([-0.6741583 , -0.01607939]), 0], [array([-0.6901465, -0.0159882]), 0], [array([-0.70593691, -0.01579042]), 1], [array([-0.72042698, -0.01449007]), 1], [array([-0.73352514, -0.01309816]), 0], [array([-0.74615088, -0.01262574]), 0], [array([-0.75822875, -0.01207787]), 2], [array([-0.76768866, -0.0094599 ]), 1], [array([-0.77547716, -0.00778851]), 1], [array([-0.78155129, -0.00607413]), 1], [array([-0.78587817, -0.00432688]), 0], [array([-0.78943474, -0.00355657]), 2], [array([-7.90202261e-01, -7.67524509e-04]), 2], [array([-0.78817673,  0.00202554]), 0], [array([-0.78536875,  0.00280798]), 2], [array([-0.77979316,  0.00557559]), 0], [array([-0.77347978,  0.00631338]), 2], [array([-0.76446296,  0.00901682]), 2], [array([-0.7527928 ,  0.01167015]), 2], [array([-0.73853597,  0.01425683]), 2], [array([-0.72177652,  0.01675945]), 2], [array([-0.70261677,  0.01915975]), 2], [array([-0.681178  ,  0.02143877]), 2], [array([-0.65760091,  0.02357709]), 2], [array([-0.63204566,  0.02555525]), 2], [array([-0.60469133,  0.02735433]), 0], [array([-0.57773479,  0.02695654]), 2], [array([-0.54937401,  0.02836078]), 1], [array([-0.52082011,  0.0285539 ]), 0], [array([-0.49328705,  0.02753306]), 2], [array([-0.46498102,  0.02830604]), 2], [array([-0.43611235,  0.02886867]), 0], [array([-0.40889233,  0.02722002]), 0], [array([-0.38351572,  0.0253766 ]), 2], [array([-0.35815909,  0.02535663]), 1], [array([-0.33399293,  0.02416615]), 1], [array([-0.31117337,  0.02281956]), 1], [array([-0.28984132,  0.02133204]), 1], [array([-0.27012226,  0.01971907]), 1], [array([-0.25212627,  0.01799599]), 0], [array([-0.2369486 ,  0.01517767]), 0], [array([-0.22466545,  0.01228315]), 2], [array([-0.21333564,  0.01132981]), 1], [array([-0.20401106,  0.00932458]), 1], [array([-0.19673268,  0.00727838]), 1], [array([-0.19153138,  0.0052013 ]), 0], [array([-0.18942861,  0.00210277]), 0], [array([-0.19043291, -0.00100429]), 0], [array([-0.1945402 , -0.00410729]), 0], [array([-0.20173367, -0.00719347]), 2], [array([-0.20998311, -0.00824944]), 1], [array([-0.2202527 , -0.01026959]), 0], [array([-0.23349611, -0.0132434 ]), 2], [array([-0.24765083, -0.01415472]), 1], [array([-0.26364674, -0.01599591]), 0], [array([-0.28240059, -0.01875385]), 2], [array([-0.30180965, -0.01940906]), 2], [array([-0.32176208, -0.01995243]), 0], [array([-0.34413746, -0.02237538]), 0], [array([-0.36879472, -0.02465726]), 2], [array([-0.39357172, -0.024777  ]), 2], [array([-0.41829938, -0.02472766]), 2], [array([-0.44280371, -0.02450433]), 1], [array([-0.4679081 , -0.02510438]), 0], [array([-0.49442822, -0.02652012]), 1], [array([-0.52116684, -0.02673862]), 1], [array([-0.5479237 , -0.02675686]), 0], [array([-0.57549828, -0.02757459]), 0], [array([-0.6036852 , -0.02818691]), 1], [array([-0.63127722, -0.02759202]), 2], [array([-0.65707563, -0.02579841]), 0], [array([-0.6828995 , -0.02582387]), 1], [array([-0.70757358, -0.02467408]), 1], [array([-0.73093684, -0.02336326]), 0], [array([-0.75384341, -0.02290658]), 1], [array([-0.77515723, -0.02131382]), 2], [array([-0.79375842, -0.01860119]), 2], [array([-0.80954807, -0.01578965]), 2], [array([-0.82244662, -0.01289855]), 0], [array([-0.83439224, -0.01194562]), 0], [array([-0.84533026, -0.01093802]), 1], [array([-0.85421288, -0.00888262]), 1], [array([-0.86100291, -0.00679003]), 1], [array([-0.86567292, -0.00467001]), 1], [array([-0.86820457, -0.00253164]), 2], [array([-8.67588065e-01,  6.16501090e-04]), 2], [array([-0.86382579,  0.00376228]), 0], [array([-0.85893235,  0.00489344]), 1], [array([-0.85192717,  0.00700518]), 2], [array([-0.84183882,  0.01008834]), 2], [array([-0.8287101 ,  0.01312872]), 0], [array([-0.81459946,  0.01411064]), 2], [array([-0.79757317,  0.01702629]), 0], [array([-0.77971574,  0.01785743]), 0], [array([-0.76112093,  0.01859481]), 0], [array([-0.74189168,  0.01922925]), 1], [array([-0.72113977,  0.02075191]), 0], [array([-0.41347394, -0.00081679]), 1], [array([-0.41510172, -0.00162778]), 1], [array([-0.41752894, -0.00242721]), 1], [array([-0.42073832, -0.00320938]), 1], [array([-0.42470697, -0.00396865]), 2], [array([-0.42840647, -0.0036995 ]), 0], [array([-0.43381025, -0.00540378]), 2], [array([-0.43887934, -0.00506908]), 0], [array([-0.44557699, -0.00669766]), 0], [array([-0.45385449, -0.00827749]), 0], [array([-0.46365126, -0.00979677]), 1], [array([-0.47389521, -0.01024395]), 2], [array([-0.48351056, -0.00961535]), 0], [array([-0.49442585, -0.01091529]), 0], [array([-0.50655966, -0.01213381]), 0], [array([-0.5198212 , -0.01326154]), 0], [array([-0.53411108, -0.01428988]), 0], [array([-0.54932213, -0.01521105]), 0], [array([-0.56534044, -0.01601831]), 2], [array([-0.58004651, -0.01470607]), 0], [array([-0.59533124, -0.01528473]), 1], [array([-0.61008212, -0.01475088]), 0], [array([-0.62519162, -0.01510951]), 1], [array([-0.63955093, -0.0143593 ]), 0], [array([-0.65405802, -0.01450709]), 0], [array([-0.66861146, -0.01455344]), 0], [array([-0.68311128, -0.01449983]), 0], [array([-0.69745991, -0.01434862]), 1], [array([-0.71056289, -0.01310299]), 1], [array([-0.72233603, -0.01177313]), 1], [array([-0.73270538, -0.01036935]), 1], [array([-0.74160729, -0.00890191]), 1], [array([-0.74898824, -0.00738095]), 0], [array([-0.75580466, -0.00681642]), 0], [array([-0.76201701, -0.00621236]), 1], [array([-0.76658984, -0.00457283]), 2], [array([-0.76849741, -0.00190757]), 2], [array([-0.76772908,  0.00076833]), 1], [array([-0.76528912,  0.00243996]), 2], [array([-0.7601912 ,  0.00509793]), 2], [array([-0.75246411,  0.00772709]), 2], [array([-0.74215224,  0.01031186]), 1], [array([-0.73031618,  0.01183607]), 1], [array([-0.71702721,  0.01328897]), 1], [array([-0.70236759,  0.01465962]), 1], [array([-0.68643056,  0.01593703]), 2], [array([-0.66832028,  0.01811028]), 2], [array([-0.64815837,  0.02016191]), 0], [array([-0.62808385,  0.02007452]), 0], [array([-0.60823847,  0.01984538]), 2], [array([-0.58676509,  0.02147338]), 2], [array([-0.56382079,  0.0229443 ]), 1], [array([-0.54057555,  0.02324523]), 1], [array([-0.51720305,  0.0233725 ]), 2], [array([-0.49287851,  0.02432454]), 1], [array([-0.46878405,  0.02409446]), 1], [array([-0.44509884,  0.0236852 ]), 1], [array([-0.42199696,  0.02310188]), 1], [array([-0.39964535,  0.02235161]), 2], [array([-0.37720211,  0.02244324]), 2], [array([-0.35482189,  0.02238022]), 2], [array([-0.3326541 ,  0.02216779]), 2], [array([-0.31084134,  0.02181275]), 0], [array([-0.29151811,  0.01932323]), 1], [array([-0.27379823,  0.01771988]), 1], [array([-0.25778134,  0.01601688]), 1], [array([-0.24355341,  0.01422794]), 1], [array([-0.23118731,  0.0123661 ]), 2], [array([-0.21974364,  0.01144367]), 1], [array([-0.21027613,  0.00946751]), 0], [array([-0.20382747,  0.00644866]), 0], [array([-0.20042581,  0.00340166]), 2], [array([-0.19808568,  0.00234013]), 2], [array([-0.19681696,  0.00126872]), 1], [array([-0.19762497, -0.00080801]), 2], [array([-0.19950632, -0.00188135]), 0], [array([-0.20445309, -0.00494678]), 0], [array([-0.21244417, -0.00799107]), 0], [array([-0.22344446, -0.01100029]), 1], [array([-0.23640378, -0.01295933]), 1], [array([-0.2512603 , -0.01485652]), 2], [array([-0.26693958, -0.01567928]), 0], [array([-0.28535916, -0.01841958]), 1], [array([-0.30541725, -0.02005809]), 2], [array([-0.32599734, -0.02058009]), 1], [array([-0.34797415, -0.02197681]), 2], [array([-0.37020804, -0.02223389]), 1], [array([-0.39355219, -0.02334415]), 2], [array([-0.41684714, -0.02329494]), 2], [array([-0.4399291 , -0.02308197]), 0], [array([-0.46463202, -0.02470292]), 0], [array([-0.49077489, -0.02614287]), 2], [array([-0.51616353, -0.02538865]), 1], [array([-0.54160794, -0.02544441]), 1], [array([-0.56691735, -0.0253094 ]), 1], [array([-0.59190278, -0.02498543]), 1], [array([-0.6163795 , -0.02447673]), 1], [array([-0.64016932, -0.02378982]), 0], [array([-0.66410258, -0.02393325]), 1], [array([-0.68701298, -0.0229104 ]), 1], [array([-0.70874629, -0.0217333 ]), 0], [array([-0.73016129, -0.02141501]), 0], [array([-0.75112435, -0.02096305]), 2], [array([-0.76951042, -0.01838607]), 2], [array([-0.78521495, -0.01570453]), 0], [array([-0.80015269, -0.01493774]), 2], [array([-0.81224617, -0.01209348]), 0], [array([-0.82343539, -0.01118922]), 2], [array([-0.83166705, -0.00823166]), 0], [array([-0.83890335, -0.00723631]), 2], [array([-0.84311208, -0.00420873]), 2], [array([-0.84427492, -0.00116284]), 2], [array([-0.84238688,  0.00188804]), 0], [array([-0.83945609,  0.00293079]), 0], [array([-0.83549529,  0.00396079]), 0], [array([-0.83052198,  0.00497332]), 0], [array([-0.8245585 ,  0.00596348]), 1], [array([-0.81663223,  0.00792627]), 2], [array([-0.80578055,  0.01085168]), 0], [array([-0.79405638,  0.01172417]), 2], [array([-0.77951912,  0.01453725]), 1], [array([-0.76324555,  0.01627357]), 2], [array([-0.74432552,  0.01892004]), 1], [array([-0.72386838,  0.02045713]), 1], [array([-0.70199798,  0.02187041]), 0], [array([-0.67985254,  0.02214543]), 1], [array([-0.65657765,  0.02327489]), 0], [array([-0.63333167,  0.02324598]), 2], [array([-0.60827747,  0.0250542 ]), 1], [array([-0.58259499,  0.02568248]), 1], [array([-0.55647235,  0.02612264]), 2], [array([-0.52910355,  0.0273688 ]), 0], [array([-0.50269347,  0.02641008]), 1], [array([-0.47644008,  0.02625339]), 0], [array([-0.4515392 ,  0.02490088]), 0], [array([-0.42817456,  0.02336463]), 2], [array([-0.40451588,  0.02365869]), 1], [array([-0.38173143,  0.02278444]), 2], [array([-0.35897916,  0.02275227]), 2], [array([-0.33641196,  0.0225672 ]), 1], [array([-0.31517603,  0.02123593]), 1], [array([-0.29540338,  0.01977265]), 1], [array([-0.27721162,  0.01819176]), 1], [array([-0.26070402,  0.01650759]), 2], [array([-0.24497   ,  0.01573403]), 0], [array([-0.2320907,  0.0128793]), 0], [array([-0.22212949,  0.0099612 ]), 2], [array([-0.21313344,  0.00899606]), 1], [array([-0.20614352,  0.00698992]), 0], [array([-0.20219057,  0.00395295]), 0], [array([-0.20129164,  0.00089893]), 1], [array([-0.20245056, -0.00115892]), 2], [array([-0.20466239, -0.00221183]), 0], [array([-0.20991762, -0.00525522]), 0], [array([-0.21819327, -0.00827566]), 0], [array([-0.22945219, -0.01125892]), 0], [array([-0.24364183, -0.01418964]), 1], [array([-0.25969287, -0.01605104]), 0], [array([-0.27852281, -0.01882994]), 2], [array([-0.29802963, -0.01950683]), 2], [array([-0.31810204, -0.0200724 ]), 0], [array([-0.34061988, -0.02251784]), 1], [array([-0.36444217, -0.02382229]), 2], [array([-0.38841329, -0.02397112]), 0], [array([-0.41437074, -0.02595745]), 0], [array([-0.44213282, -0.02776208]), 0], [array([-0.47149983, -0.02936701]), 2], [array([-0.50025599, -0.02875616]), 2], [array([-0.52818708, -0.02793109]), 2], [array([-0.55508376, -0.02689668]), 2], [array([-0.58074465, -0.02566089]), 2], [array([-0.60497905, -0.02423439]), 1], [array([-0.62860913, -0.02363009]), 0], [array([-0.65246461, -0.02385547]), 0], [array([-0.67637748, -0.02391287]), 1], [array([-0.69918421, -0.02280673]), 1], [array([-0.72073412, -0.0215499 ]), 0], [array([-0.7418902 , -0.02115608]), 2], [array([-0.76052363, -0.01863343]), 1], [array([-0.77752601, -0.01700238]), 1], [array([-0.79280286, -0.01527685]), 1], [array([-0.80627311, -0.01347025]), 1], [array([-0.81786843, -0.01159532]), 2], [array([-0.82653243, -0.008664  ]), 1], [array([-0.83322452, -0.00669209]), 0], [array([-0.83891424, -0.00568972]), 1], [array([-0.84257632, -0.00366209]), 2], [array([-8.43194841e-01, -6.18516763e-04]), 1], [array([-0.84176712,  0.00142772]), 1], [array([-0.83829932,  0.00346779]), 0], [array([-0.83380661,  0.00449272]), 2], [array([-0.82630891,  0.0074977 ]), 0], [array([-0.81784032,  0.00846859]), 2], [array([-0.80644055,  0.01139977]), 1], [array([-0.79316502,  0.01327553]), 0], [array([-0.77908101,  0.01408401]), 1], [array([-0.76326306,  0.01581796]), 1], [array([-0.74579853,  0.01746452]), 2], [array([-0.72578822,  0.02001032]), 0]]\n"
     ]
    }
   ],
   "source": [
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4eae184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "392/392 [==============================] - 1s 1ms/step - loss: 1.0990 - accuracy: 0.3445 - val_loss: 1.0979 - val_accuracy: 0.3369\n",
      "Epoch 2/300\n",
      "392/392 [==============================] - 0s 992us/step - loss: 1.0965 - accuracy: 0.3642 - val_loss: 1.0938 - val_accuracy: 0.3836\n",
      "Epoch 3/300\n",
      "392/392 [==============================] - 0s 956us/step - loss: 1.0927 - accuracy: 0.3900 - val_loss: 1.0879 - val_accuracy: 0.4006\n",
      "Epoch 4/300\n",
      "392/392 [==============================] - 0s 847us/step - loss: 1.0877 - accuracy: 0.4039 - val_loss: 1.0857 - val_accuracy: 0.3994\n",
      "Epoch 5/300\n",
      "392/392 [==============================] - 0s 894us/step - loss: 1.0852 - accuracy: 0.4018 - val_loss: 1.0852 - val_accuracy: 0.4013\n",
      "Epoch 6/300\n",
      "392/392 [==============================] - 0s 830us/step - loss: 1.0858 - accuracy: 0.4031 - val_loss: 1.0847 - val_accuracy: 0.4013\n",
      "Epoch 7/300\n",
      "392/392 [==============================] - 0s 900us/step - loss: 1.0853 - accuracy: 0.4035 - val_loss: 1.0850 - val_accuracy: 0.4025\n",
      "Epoch 8/300\n",
      "392/392 [==============================] - 0s 974us/step - loss: 1.0861 - accuracy: 0.3977 - val_loss: 1.0864 - val_accuracy: 0.3946\n",
      "Epoch 9/300\n",
      "392/392 [==============================] - 0s 966us/step - loss: 1.0829 - accuracy: 0.4098 - val_loss: 1.0843 - val_accuracy: 0.4020\n",
      "Epoch 10/300\n",
      "392/392 [==============================] - 0s 989us/step - loss: 1.0847 - accuracy: 0.4034 - val_loss: 1.0841 - val_accuracy: 0.4015\n",
      "Epoch 11/300\n",
      "392/392 [==============================] - 0s 947us/step - loss: 1.0843 - accuracy: 0.4045 - val_loss: 1.0843 - val_accuracy: 0.4015\n",
      "Epoch 12/300\n",
      "392/392 [==============================] - 0s 973us/step - loss: 1.0839 - accuracy: 0.4073 - val_loss: 1.0854 - val_accuracy: 0.3960\n",
      "Epoch 13/300\n",
      "392/392 [==============================] - 0s 1ms/step - loss: 1.0861 - accuracy: 0.3987 - val_loss: 1.0835 - val_accuracy: 0.4013\n",
      "Epoch 14/300\n",
      "392/392 [==============================] - 0s 1ms/step - loss: 1.0843 - accuracy: 0.4040 - val_loss: 1.0855 - val_accuracy: 0.3958\n",
      "Epoch 15/300\n",
      "392/392 [==============================] - 0s 1ms/step - loss: 1.0818 - accuracy: 0.4079 - val_loss: 1.0838 - val_accuracy: 0.4022\n",
      "Epoch 16/300\n",
      "392/392 [==============================] - 0s 992us/step - loss: 1.0834 - accuracy: 0.4023 - val_loss: 1.0831 - val_accuracy: 0.3994\n",
      "Epoch 17/300\n",
      "392/392 [==============================] - 0s 966us/step - loss: 1.0835 - accuracy: 0.4016 - val_loss: 1.0825 - val_accuracy: 0.4025\n",
      "Epoch 18/300\n",
      "392/392 [==============================] - 0s 951us/step - loss: 1.0821 - accuracy: 0.4060 - val_loss: 1.0829 - val_accuracy: 0.4034\n",
      "Epoch 19/300\n",
      "392/392 [==============================] - 0s 926us/step - loss: 1.0817 - accuracy: 0.4057 - val_loss: 1.0826 - val_accuracy: 0.4022\n",
      "Epoch 20/300\n",
      "392/392 [==============================] - 0s 958us/step - loss: 1.0828 - accuracy: 0.4021 - val_loss: 1.0832 - val_accuracy: 0.3987\n",
      "Epoch 21/300\n",
      "392/392 [==============================] - 0s 839us/step - loss: 1.0836 - accuracy: 0.4007 - val_loss: 1.0827 - val_accuracy: 0.4015\n",
      "Epoch 22/300\n",
      "392/392 [==============================] - 0s 939us/step - loss: 1.0826 - accuracy: 0.4007 - val_loss: 1.0818 - val_accuracy: 0.4020\n",
      "Epoch 23/300\n",
      "392/392 [==============================] - 0s 980us/step - loss: 1.0809 - accuracy: 0.4039 - val_loss: 1.0819 - val_accuracy: 0.3999\n",
      "Epoch 24/300\n",
      "392/392 [==============================] - 0s 998us/step - loss: 1.0805 - accuracy: 0.4049 - val_loss: 1.0823 - val_accuracy: 0.3999\n",
      "Epoch 25/300\n",
      "392/392 [==============================] - 0s 954us/step - loss: 1.0828 - accuracy: 0.4057 - val_loss: 1.0835 - val_accuracy: 0.3960\n",
      "Epoch 26/300\n",
      "392/392 [==============================] - 0s 933us/step - loss: 1.0829 - accuracy: 0.4013 - val_loss: 1.0823 - val_accuracy: 0.3982\n",
      "Epoch 27/300\n",
      "392/392 [==============================] - 0s 1ms/step - loss: 1.0790 - accuracy: 0.4093 - val_loss: 1.0826 - val_accuracy: 0.3972\n"
     ]
    }
   ],
   "source": [
    "train_X = np.array([i[0] for i in training_data]).reshape(-1, 2) \n",
    "train_Y = np.array([i[1] for i in training_data]).reshape(-1, 1)\n",
    "model = tf.keras.Sequential([ \n",
    "    tf.keras.layers.Dense(128, input_shape=(2,), activation='relu'), \n",
    "    tf.keras.layers.Dense(32, activation='relu'), \n",
    "    tf.keras.layers.Dense(3, activation='softmax') \n",
    "]) \n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(), \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ") \n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5) \n",
    "history = model.fit(train_X, train_Y, epochs=30, callbacks=[callback], batch_size=32, validation_split=0.25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bac48237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(history.history['accuracy'], 'b-', label='accuracy') \n",
    "plt.plot(history.history['val_accuracy'], 'k--', label='val_accuracy') \n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eec3ca83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score :  -118.0\n",
      "step :  118\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env.close()\n",
    "env.reset()\n",
    "\n",
    "score = 0 \n",
    "step = 0 \n",
    "previous_obs = [] \n",
    "while True: \n",
    "    if len(previous_obs) == 0: \n",
    "        action = env.action_space.sample() \n",
    "    else: \n",
    "        logit = model.predict(np.expand_dims(previous_obs, axis=0))[0] \n",
    "        action = np.argmax(logit) \n",
    "    obs, reward, done, _ = env.step(action) \n",
    "    previous_obs = obs \n",
    "    score += reward \n",
    "    step += 1 \n",
    "    env.render()\n",
    "    if done: \n",
    "        break \n",
    "        \n",
    "print('score : ', score) \n",
    "print('step : ', step) \n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a430e045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 Average Reward: -200.0\n",
      "Episode 200 Average Reward: -200.0\n",
      "Episode 300 Average Reward: -200.0\n",
      "Episode 400 Average Reward: -200.0\n",
      "Episode 500 Average Reward: -200.0\n",
      "Episode 600 Average Reward: -200.0\n",
      "Episode 700 Average Reward: -200.0\n",
      "Episode 800 Average Reward: -200.0\n",
      "Episode 900 Average Reward: -200.0\n",
      "Episode 1000 Average Reward: -200.0\n",
      "Episode 1100 Average Reward: -200.0\n",
      "Episode 1200 Average Reward: -200.0\n",
      "Episode 1300 Average Reward: -200.0\n",
      "Episode 1400 Average Reward: -200.0\n",
      "Episode 1500 Average Reward: -200.0\n",
      "Episode 1600 Average Reward: -200.0\n",
      "Episode 1700 Average Reward: -200.0\n",
      "Episode 1800 Average Reward: -200.0\n",
      "Episode 1900 Average Reward: -200.0\n",
      "Episode 2000 Average Reward: -200.0\n",
      "Episode 2100 Average Reward: -200.0\n",
      "Episode 2200 Average Reward: -200.0\n",
      "Episode 2300 Average Reward: -200.0\n",
      "Episode 2400 Average Reward: -200.0\n",
      "Episode 2500 Average Reward: -200.0\n",
      "Episode 2600 Average Reward: -200.0\n",
      "Episode 2700 Average Reward: -200.0\n",
      "Episode 2800 Average Reward: -200.0\n",
      "Episode 2900 Average Reward: -200.0\n",
      "Episode 3000 Average Reward: -200.0\n",
      "Episode 3100 Average Reward: -200.0\n",
      "Episode 3200 Average Reward: -200.0\n",
      "Episode 3300 Average Reward: -200.0\n",
      "Episode 3400 Average Reward: -200.0\n",
      "Episode 3500 Average Reward: -200.0\n",
      "Episode 3600 Average Reward: -200.0\n",
      "Episode 3700 Average Reward: -200.0\n",
      "Episode 3800 Average Reward: -200.0\n",
      "Episode 3900 Average Reward: -200.0\n",
      "Episode 4000 Average Reward: -200.0\n",
      "Episode 4100 Average Reward: -200.0\n",
      "Episode 4200 Average Reward: -200.0\n",
      "Episode 4300 Average Reward: -200.0\n",
      "Episode 4400 Average Reward: -200.0\n",
      "Episode 4500 Average Reward: -200.0\n",
      "Episode 4600 Average Reward: -200.0\n",
      "Episode 4700 Average Reward: -200.0\n",
      "Episode 4800 Average Reward: -200.0\n",
      "Episode 4900 Average Reward: -200.0\n",
      "Episode 5000 Average Reward: -200.0\n",
      "Episode 5100 Average Reward: -200.0\n",
      "Episode 5200 Average Reward: -200.0\n",
      "Episode 5300 Average Reward: -200.0\n",
      "Episode 5400 Average Reward: -200.0\n",
      "Episode 5500 Average Reward: -200.0\n",
      "Episode 5600 Average Reward: -200.0\n",
      "Episode 5700 Average Reward: -200.0\n",
      "Episode 5800 Average Reward: -200.0\n",
      "Episode 5900 Average Reward: -200.0\n",
      "Episode 6000 Average Reward: -200.0\n",
      "Episode 6100 Average Reward: -200.0\n",
      "Episode 6200 Average Reward: -200.0\n",
      "Episode 6300 Average Reward: -200.0\n",
      "Episode 6400 Average Reward: -200.0\n",
      "Episode 6500 Average Reward: -200.0\n",
      "Episode 6600 Average Reward: -200.0\n",
      "Episode 6700 Average Reward: -200.0\n",
      "Episode 6800 Average Reward: -200.0\n",
      "Episode 6900 Average Reward: -200.0\n",
      "Episode 7000 Average Reward: -200.0\n",
      "Episode 7100 Average Reward: -200.0\n",
      "Episode 7200 Average Reward: -200.0\n",
      "Episode 7300 Average Reward: -200.0\n",
      "Episode 7400 Average Reward: -200.0\n",
      "Episode 7500 Average Reward: -200.0\n",
      "Episode 7600 Average Reward: -200.0\n",
      "Episode 7700 Average Reward: -200.0\n",
      "Episode 7800 Average Reward: -200.0\n",
      "Episode 7900 Average Reward: -200.0\n",
      "Episode 8000 Average Reward: -200.0\n",
      "Episode 8100 Average Reward: -200.0\n",
      "Episode 8200 Average Reward: -200.0\n",
      "Episode 8300 Average Reward: -200.0\n",
      "Episode 8400 Average Reward: -200.0\n",
      "Episode 8500 Average Reward: -200.0\n",
      "Episode 8600 Average Reward: -200.0\n",
      "Episode 8700 Average Reward: -200.0\n",
      "Episode 8800 Average Reward: -200.0\n",
      "Episode 8900 Average Reward: -200.0\n",
      "Episode 9000 Average Reward: -200.0\n",
      "Episode 9100 Average Reward: -200.0\n",
      "Episode 9200 Average Reward: -200.0\n",
      "Episode 9300 Average Reward: -200.0\n",
      "Episode 9400 Average Reward: -200.0\n",
      "Episode 9500 Average Reward: -200.0\n",
      "Episode 9600 Average Reward: -200.0\n",
      "Episode 9700 Average Reward: -200.0\n",
      "Episode 9800 Average Reward: -200.0\n",
      "Episode 9900 Average Reward: -200.0\n",
      "Episode 10000 Average Reward: -200.0\n",
      "Episode 10100 Average Reward: -200.0\n",
      "Episode 10200 Average Reward: -200.0\n",
      "Episode 10300 Average Reward: -200.0\n",
      "Episode 10400 Average Reward: -200.0\n",
      "Episode 10500 Average Reward: -200.0\n",
      "Episode 10600 Average Reward: -200.0\n",
      "Episode 10700 Average Reward: -200.0\n",
      "Episode 10800 Average Reward: -200.0\n",
      "Episode 10900 Average Reward: -200.0\n",
      "Episode 11000 Average Reward: -200.0\n",
      "Episode 11100 Average Reward: -200.0\n",
      "Episode 11200 Average Reward: -200.0\n",
      "Episode 11300 Average Reward: -200.0\n",
      "Episode 11400 Average Reward: -200.0\n",
      "Episode 11500 Average Reward: -200.0\n",
      "Episode 11600 Average Reward: -200.0\n",
      "Episode 11700 Average Reward: -200.0\n",
      "Episode 11800 Average Reward: -200.0\n",
      "Episode 11900 Average Reward: -200.0\n",
      "Episode 12000 Average Reward: -200.0\n",
      "Episode 12100 Average Reward: -200.0\n",
      "Episode 12200 Average Reward: -200.0\n",
      "Episode 12300 Average Reward: -200.0\n",
      "Episode 12400 Average Reward: -200.0\n",
      "Episode 12500 Average Reward: -200.0\n",
      "Episode 12600 Average Reward: -200.0\n",
      "Episode 12700 Average Reward: -200.0\n",
      "Episode 12800 Average Reward: -200.0\n",
      "Episode 12900 Average Reward: -200.0\n",
      "Episode 13000 Average Reward: -200.0\n",
      "Episode 13100 Average Reward: -200.0\n",
      "Episode 13200 Average Reward: -200.0\n",
      "Episode 13300 Average Reward: -200.0\n",
      "Episode 13400 Average Reward: -200.0\n",
      "Episode 13500 Average Reward: -200.0\n",
      "Episode 13600 Average Reward: -200.0\n",
      "Episode 13700 Average Reward: -200.0\n",
      "Episode 13800 Average Reward: -200.0\n",
      "Episode 13900 Average Reward: -200.0\n",
      "Episode 14000 Average Reward: -200.0\n",
      "Episode 14100 Average Reward: -200.0\n",
      "Episode 14200 Average Reward: -200.0\n",
      "Episode 14300 Average Reward: -200.0\n",
      "Episode 14400 Average Reward: -200.0\n",
      "Episode 14500 Average Reward: -200.0\n",
      "Episode 14600 Average Reward: -200.0\n",
      "Episode 14700 Average Reward: -200.0\n",
      "Episode 14800 Average Reward: -200.0\n",
      "Episode 14900 Average Reward: -200.0\n",
      "Episode 15000 Average Reward: -200.0\n",
      "Episode 15100 Average Reward: -200.0\n",
      "Episode 15200 Average Reward: -200.0\n",
      "Episode 15300 Average Reward: -200.0\n",
      "Episode 15400 Average Reward: -200.0\n",
      "Episode 15500 Average Reward: -200.0\n",
      "Episode 15600 Average Reward: -200.0\n",
      "Episode 15700 Average Reward: -200.0\n",
      "Episode 15800 Average Reward: -200.0\n",
      "Episode 15900 Average Reward: -200.0\n",
      "Episode 16000 Average Reward: -200.0\n",
      "Episode 16100 Average Reward: -200.0\n",
      "Episode 16200 Average Reward: -200.0\n",
      "Episode 16300 Average Reward: -200.0\n",
      "Episode 16400 Average Reward: -200.0\n",
      "Episode 16500 Average Reward: -200.0\n",
      "Episode 16600 Average Reward: -200.0\n",
      "Episode 16700 Average Reward: -200.0\n",
      "Episode 16800 Average Reward: -200.0\n",
      "Episode 16900 Average Reward: -200.0\n",
      "Episode 17000 Average Reward: -200.0\n",
      "Episode 17100 Average Reward: -200.0\n",
      "Episode 17200 Average Reward: -200.0\n",
      "Episode 17300 Average Reward: -200.0\n",
      "Episode 17400 Average Reward: -200.0\n",
      "Episode 17500 Average Reward: -200.0\n",
      "Episode 17600 Average Reward: -200.0\n",
      "Episode 17700 Average Reward: -200.0\n",
      "Episode 17800 Average Reward: -200.0\n",
      "Episode 17900 Average Reward: -200.0\n",
      "Episode 18000 Average Reward: -200.0\n",
      "Episode 18100 Average Reward: -200.0\n",
      "Episode 18200 Average Reward: -200.0\n",
      "Episode 18300 Average Reward: -200.0\n",
      "Episode 18400 Average Reward: -200.0\n",
      "Episode 18500 Average Reward: -200.0\n",
      "Episode 18600 Average Reward: -200.0\n",
      "Episode 18700 Average Reward: -200.0\n",
      "Episode 18800 Average Reward: -200.0\n",
      "Episode 18900 Average Reward: -200.0\n",
      "Episode 19000 Average Reward: -200.0\n",
      "Episode 19100 Average Reward: -200.0\n",
      "Episode 19200 Average Reward: -200.0\n",
      "Episode 19300 Average Reward: -200.0\n",
      "Episode 19400 Average Reward: -200.0\n",
      "Episode 19500 Average Reward: -200.0\n",
      "Episode 19600 Average Reward: -200.0\n",
      "Episode 19700 Average Reward: -200.0\n",
      "Episode 19800 Average Reward: -200.0\n",
      "Episode 19900 Average Reward: -200.0\n",
      "Episode 20000 Average Reward: -200.0\n",
      "Episode 20100 Average Reward: -200.0\n",
      "Episode 20200 Average Reward: -200.0\n",
      "Episode 20300 Average Reward: -200.0\n",
      "Episode 20400 Average Reward: -200.0\n",
      "Episode 20500 Average Reward: -200.0\n",
      "Episode 20600 Average Reward: -200.0\n",
      "Episode 20700 Average Reward: -200.0\n",
      "Episode 20800 Average Reward: -200.0\n",
      "Episode 20900 Average Reward: -200.0\n",
      "Episode 21000 Average Reward: -200.0\n",
      "Episode 21100 Average Reward: -200.0\n",
      "Episode 21200 Average Reward: -200.0\n",
      "Episode 21300 Average Reward: -200.0\n",
      "Episode 21400 Average Reward: -200.0\n",
      "Episode 21500 Average Reward: -199.7\n",
      "Episode 21600 Average Reward: -200.0\n",
      "Episode 21700 Average Reward: -200.0\n",
      "Episode 21800 Average Reward: -200.0\n",
      "Episode 21900 Average Reward: -200.0\n",
      "Episode 22000 Average Reward: -200.0\n",
      "Episode 22100 Average Reward: -200.0\n",
      "Episode 22200 Average Reward: -200.0\n",
      "Episode 22300 Average Reward: -199.94\n",
      "Episode 22400 Average Reward: -200.0\n",
      "Episode 22500 Average Reward: -200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 22600 Average Reward: -200.0\n",
      "Episode 22700 Average Reward: -200.0\n",
      "Episode 22800 Average Reward: -200.0\n",
      "Episode 22900 Average Reward: -200.0\n",
      "Episode 23000 Average Reward: -200.0\n",
      "Episode 23100 Average Reward: -200.0\n",
      "Episode 23200 Average Reward: -200.0\n",
      "Episode 23300 Average Reward: -200.0\n",
      "Episode 23400 Average Reward: -200.0\n",
      "Episode 23500 Average Reward: -200.0\n",
      "Episode 23600 Average Reward: -200.0\n",
      "Episode 23700 Average Reward: -200.0\n",
      "Episode 23800 Average Reward: -200.0\n",
      "Episode 23900 Average Reward: -200.0\n",
      "Episode 24000 Average Reward: -199.99\n",
      "Episode 24100 Average Reward: -199.88\n",
      "Episode 24200 Average Reward: -200.0\n",
      "Episode 24300 Average Reward: -200.0\n",
      "Episode 24400 Average Reward: -200.0\n",
      "Episode 24500 Average Reward: -200.0\n",
      "Episode 24600 Average Reward: -199.8\n",
      "Episode 24700 Average Reward: -200.0\n",
      "Episode 24800 Average Reward: -199.93\n",
      "Episode 24900 Average Reward: -200.0\n",
      "Episode 25000 Average Reward: -199.9\n",
      "Episode 25100 Average Reward: -200.0\n",
      "Episode 25200 Average Reward: -200.0\n",
      "Episode 25300 Average Reward: -200.0\n",
      "Episode 25400 Average Reward: -199.89\n",
      "Episode 25500 Average Reward: -199.99\n",
      "Episode 25600 Average Reward: -200.0\n",
      "Episode 25700 Average Reward: -200.0\n",
      "Episode 25800 Average Reward: -200.0\n",
      "Episode 25900 Average Reward: -200.0\n",
      "Episode 26000 Average Reward: -199.98\n",
      "Episode 26100 Average Reward: -200.0\n",
      "Episode 26200 Average Reward: -200.0\n",
      "Episode 26300 Average Reward: -200.0\n",
      "Episode 26400 Average Reward: -200.0\n",
      "Episode 26500 Average Reward: -200.0\n",
      "Episode 26600 Average Reward: -200.0\n",
      "Episode 26700 Average Reward: -199.73\n",
      "Episode 26800 Average Reward: -200.0\n",
      "Episode 26900 Average Reward: -200.0\n",
      "Episode 27000 Average Reward: -200.0\n",
      "Episode 27100 Average Reward: -200.0\n",
      "Episode 27200 Average Reward: -199.71\n",
      "Episode 27300 Average Reward: -199.54\n",
      "Episode 27400 Average Reward: -200.0\n",
      "Episode 27500 Average Reward: -200.0\n",
      "Episode 27600 Average Reward: -199.21\n",
      "Episode 27700 Average Reward: -200.0\n",
      "Episode 27800 Average Reward: -200.0\n",
      "Episode 27900 Average Reward: -199.76\n",
      "Episode 28000 Average Reward: -200.0\n",
      "Episode 28100 Average Reward: -199.97\n",
      "Episode 28200 Average Reward: -200.0\n",
      "Episode 28300 Average Reward: -200.0\n",
      "Episode 28400 Average Reward: -200.0\n",
      "Episode 28500 Average Reward: -199.98\n",
      "Episode 28600 Average Reward: -199.9\n",
      "Episode 28700 Average Reward: -200.0\n",
      "Episode 28800 Average Reward: -199.88\n",
      "Episode 28900 Average Reward: -200.0\n",
      "Episode 29000 Average Reward: -200.0\n",
      "Episode 29100 Average Reward: -200.0\n",
      "Episode 29200 Average Reward: -200.0\n",
      "Episode 29300 Average Reward: -200.0\n",
      "Episode 29400 Average Reward: -200.0\n",
      "Episode 29500 Average Reward: -200.0\n",
      "Episode 29600 Average Reward: -200.0\n",
      "Episode 29700 Average Reward: -199.68\n",
      "Episode 29800 Average Reward: -199.57\n",
      "Episode 29900 Average Reward: -199.98\n",
      "Episode 30000 Average Reward: -200.0\n",
      "Episode 30100 Average Reward: -200.0\n",
      "Episode 30200 Average Reward: -200.0\n",
      "Episode 30300 Average Reward: -200.0\n",
      "Episode 30400 Average Reward: -199.42\n",
      "Episode 30500 Average Reward: -199.54\n",
      "Episode 30600 Average Reward: -200.0\n",
      "Episode 30700 Average Reward: -199.46\n",
      "Episode 30800 Average Reward: -199.23\n",
      "Episode 30900 Average Reward: -198.29\n",
      "Episode 31000 Average Reward: -199.39\n",
      "Episode 31100 Average Reward: -200.0\n",
      "Episode 31200 Average Reward: -199.62\n",
      "Episode 31300 Average Reward: -199.89\n",
      "Episode 31400 Average Reward: -200.0\n",
      "Episode 31500 Average Reward: -200.0\n",
      "Episode 31600 Average Reward: -200.0\n",
      "Episode 31700 Average Reward: -200.0\n",
      "Episode 31800 Average Reward: -199.9\n",
      "Episode 31900 Average Reward: -199.88\n",
      "Episode 32000 Average Reward: -200.0\n",
      "Episode 32100 Average Reward: -200.0\n",
      "Episode 32200 Average Reward: -199.29\n",
      "Episode 32300 Average Reward: -200.0\n",
      "Episode 32400 Average Reward: -199.92\n",
      "Episode 32500 Average Reward: -199.45\n",
      "Episode 32600 Average Reward: -199.9\n",
      "Episode 32700 Average Reward: -200.0\n",
      "Episode 32800 Average Reward: -198.6\n",
      "Episode 32900 Average Reward: -198.49\n",
      "Episode 33000 Average Reward: -199.72\n",
      "Episode 33100 Average Reward: -200.0\n",
      "Episode 33200 Average Reward: -200.0\n",
      "Episode 33300 Average Reward: -200.0\n",
      "Episode 33400 Average Reward: -200.0\n",
      "Episode 33500 Average Reward: -199.69\n",
      "Episode 33600 Average Reward: -198.98\n",
      "Episode 33700 Average Reward: -198.32\n",
      "Episode 33800 Average Reward: -197.82\n",
      "Episode 33900 Average Reward: -200.0\n",
      "Episode 34000 Average Reward: -200.0\n",
      "Episode 34100 Average Reward: -199.55\n",
      "Episode 34200 Average Reward: -198.7\n",
      "Episode 34300 Average Reward: -199.01\n",
      "Episode 34400 Average Reward: -200.0\n",
      "Episode 34500 Average Reward: -199.36\n",
      "Episode 34600 Average Reward: -193.47\n",
      "Episode 34700 Average Reward: -197.08\n",
      "Episode 34800 Average Reward: -199.32\n",
      "Episode 34900 Average Reward: -199.23\n",
      "Episode 35000 Average Reward: -199.36\n",
      "Episode 35100 Average Reward: -197.39\n",
      "Episode 35200 Average Reward: -194.61\n",
      "Episode 35300 Average Reward: -198.01\n",
      "Episode 35400 Average Reward: -199.89\n",
      "Episode 35500 Average Reward: -198.8\n",
      "Episode 35600 Average Reward: -199.83\n",
      "Episode 35700 Average Reward: -199.18\n",
      "Episode 35800 Average Reward: -197.66\n",
      "Episode 35900 Average Reward: -198.0\n",
      "Episode 36000 Average Reward: -199.26\n",
      "Episode 36100 Average Reward: -198.92\n",
      "Episode 36200 Average Reward: -198.6\n",
      "Episode 36300 Average Reward: -198.07\n",
      "Episode 36400 Average Reward: -196.19\n",
      "Episode 36500 Average Reward: -200.0\n",
      "Episode 36600 Average Reward: -198.8\n",
      "Episode 36700 Average Reward: -198.95\n",
      "Episode 36800 Average Reward: -199.61\n",
      "Episode 36900 Average Reward: -197.67\n",
      "Episode 37000 Average Reward: -197.66\n",
      "Episode 37100 Average Reward: -197.54\n",
      "Episode 37200 Average Reward: -195.69\n",
      "Episode 37300 Average Reward: -199.25\n",
      "Episode 37400 Average Reward: -199.91\n",
      "Episode 37500 Average Reward: -197.84\n",
      "Episode 37600 Average Reward: -195.76\n",
      "Episode 37700 Average Reward: -196.8\n",
      "Episode 37800 Average Reward: -194.99\n",
      "Episode 37900 Average Reward: -199.97\n",
      "Episode 38000 Average Reward: -200.0\n",
      "Episode 38100 Average Reward: -200.0\n",
      "Episode 38200 Average Reward: -199.58\n",
      "Episode 38300 Average Reward: -198.15\n",
      "Episode 38400 Average Reward: -195.22\n",
      "Episode 38500 Average Reward: -199.93\n",
      "Episode 38600 Average Reward: -197.9\n",
      "Episode 38700 Average Reward: -195.36\n",
      "Episode 38800 Average Reward: -195.21\n",
      "Episode 38900 Average Reward: -196.78\n",
      "Episode 39000 Average Reward: -196.43\n",
      "Episode 39100 Average Reward: -198.82\n",
      "Episode 39200 Average Reward: -197.11\n",
      "Episode 39300 Average Reward: -197.71\n",
      "Episode 39400 Average Reward: -199.59\n",
      "Episode 39500 Average Reward: -199.4\n",
      "Episode 39600 Average Reward: -198.23\n",
      "Episode 39700 Average Reward: -189.81\n",
      "Episode 39800 Average Reward: -196.31\n",
      "Episode 39900 Average Reward: -198.43\n",
      "Episode 40000 Average Reward: -199.41\n",
      "Episode 40100 Average Reward: -197.76\n",
      "Episode 40200 Average Reward: -197.82\n",
      "Episode 40300 Average Reward: -195.17\n",
      "Episode 40400 Average Reward: -192.7\n",
      "Episode 40500 Average Reward: -199.56\n",
      "Episode 40600 Average Reward: -200.0\n",
      "Episode 40700 Average Reward: -199.6\n",
      "Episode 40800 Average Reward: -197.5\n",
      "Episode 40900 Average Reward: -199.73\n",
      "Episode 41000 Average Reward: -196.37\n",
      "Episode 41100 Average Reward: -192.83\n",
      "Episode 41200 Average Reward: -196.22\n",
      "Episode 41300 Average Reward: -198.29\n",
      "Episode 41400 Average Reward: -197.11\n",
      "Episode 41500 Average Reward: -196.61\n",
      "Episode 41600 Average Reward: -190.97\n",
      "Episode 41700 Average Reward: -194.9\n",
      "Episode 41800 Average Reward: -198.72\n",
      "Episode 41900 Average Reward: -194.99\n",
      "Episode 42000 Average Reward: -198.02\n",
      "Episode 42100 Average Reward: -198.43\n",
      "Episode 42200 Average Reward: -193.61\n",
      "Episode 42300 Average Reward: -190.6\n",
      "Episode 42400 Average Reward: -195.99\n",
      "Episode 42500 Average Reward: -195.15\n",
      "Episode 42600 Average Reward: -190.58\n",
      "Episode 42700 Average Reward: -196.2\n",
      "Episode 42800 Average Reward: -197.86\n",
      "Episode 42900 Average Reward: -198.27\n",
      "Episode 43000 Average Reward: -196.71\n",
      "Episode 43100 Average Reward: -191.72\n",
      "Episode 43200 Average Reward: -187.29\n",
      "Episode 43300 Average Reward: -194.39\n",
      "Episode 43400 Average Reward: -199.77\n",
      "Episode 43500 Average Reward: -199.66\n",
      "Episode 43600 Average Reward: -198.12\n",
      "Episode 43700 Average Reward: -193.35\n",
      "Episode 43800 Average Reward: -195.61\n",
      "Episode 43900 Average Reward: -198.79\n",
      "Episode 44000 Average Reward: -191.98\n",
      "Episode 44100 Average Reward: -193.89\n",
      "Episode 44200 Average Reward: -188.18\n",
      "Episode 44300 Average Reward: -177.55\n",
      "Episode 44400 Average Reward: -195.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 44500 Average Reward: -196.96\n",
      "Episode 44600 Average Reward: -196.27\n",
      "Episode 44700 Average Reward: -188.65\n",
      "Episode 44800 Average Reward: -181.76\n",
      "Episode 44900 Average Reward: -190.65\n",
      "Episode 45000 Average Reward: -190.58\n",
      "Episode 45100 Average Reward: -190.64\n",
      "Episode 45200 Average Reward: -187.31\n",
      "Episode 45300 Average Reward: -185.3\n",
      "Episode 45400 Average Reward: -193.09\n",
      "Episode 45500 Average Reward: -175.43\n",
      "Episode 45600 Average Reward: -178.82\n",
      "Episode 45700 Average Reward: -190.09\n",
      "Episode 45800 Average Reward: -198.58\n",
      "Episode 45900 Average Reward: -195.47\n",
      "Episode 46000 Average Reward: -186.16\n",
      "Episode 46100 Average Reward: -186.34\n",
      "Episode 46200 Average Reward: -193.18\n",
      "Episode 46300 Average Reward: -196.52\n",
      "Episode 46400 Average Reward: -181.37\n",
      "Episode 46500 Average Reward: -197.02\n",
      "Episode 46600 Average Reward: -188.48\n",
      "Episode 46700 Average Reward: -194.07\n",
      "Episode 46800 Average Reward: -197.34\n",
      "Episode 46900 Average Reward: -197.82\n",
      "Episode 47000 Average Reward: -191.88\n",
      "Episode 47100 Average Reward: -192.35\n",
      "Episode 47200 Average Reward: -193.59\n",
      "Episode 47300 Average Reward: -195.28\n",
      "Episode 47400 Average Reward: -187.78\n",
      "Episode 47500 Average Reward: -191.86\n",
      "Episode 47600 Average Reward: -183.58\n",
      "Episode 47700 Average Reward: -171.87\n",
      "Episode 47800 Average Reward: -186.17\n",
      "Episode 47900 Average Reward: -190.8\n",
      "Episode 48000 Average Reward: -189.87\n",
      "Episode 48100 Average Reward: -179.64\n",
      "Episode 48200 Average Reward: -190.04\n",
      "Episode 48300 Average Reward: -183.57\n",
      "Episode 48400 Average Reward: -177.04\n",
      "Episode 48500 Average Reward: -179.18\n",
      "Episode 48600 Average Reward: -178.84\n",
      "Episode 48700 Average Reward: -185.08\n",
      "Episode 48800 Average Reward: -174.95\n",
      "Episode 48900 Average Reward: -170.05\n",
      "Episode 49000 Average Reward: -183.56\n",
      "Episode 49100 Average Reward: -179.06\n",
      "Episode 49200 Average Reward: -184.71\n",
      "Episode 49300 Average Reward: -192.6\n",
      "Episode 49400 Average Reward: -174.53\n",
      "Episode 49500 Average Reward: -165.99\n",
      "Episode 49600 Average Reward: -179.86\n",
      "Episode 49700 Average Reward: -175.54\n",
      "Episode 49800 Average Reward: -160.53\n",
      "Episode 49900 Average Reward: -162.05\n",
      "Episode 50000 Average Reward: -146.77\n"
     ]
    }
   ],
   "source": [
    "#https://gist.github.com/gkhayes/3d154e0505e31d6367be22ed3da2e955\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import and initialize Mountain Car Environment\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "\n",
    "# Define Q-learning function\n",
    "def QLearning(env, learning, discount, epsilon, min_eps, episodes):\n",
    "    # Determine size of discretized state space\n",
    "    num_states = (env.observation_space.high - env.observation_space.low)*\\\n",
    "                    np.array([10, 100])\n",
    "    num_states = np.round(num_states, 0).astype(int) + 1\n",
    "    \n",
    "    # Initialize Q table\n",
    "    Q = np.random.uniform(low = -1, high = 1, \n",
    "                          size = (num_states[0], num_states[1], \n",
    "                                  env.action_space.n))\n",
    "    \n",
    "    # Initialize variables to track rewards\n",
    "    reward_list = []\n",
    "    ave_reward_list = []\n",
    "    \n",
    "    # Calculate episodic reduction in epsilon\n",
    "    reduction = (epsilon - min_eps)/episodes\n",
    "    \n",
    "    # Run Q learning algorithm\n",
    "    for i in range(episodes):\n",
    "        # Initialize parameters\n",
    "        done = False\n",
    "        tot_reward, reward = 0,0\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Discretize state\n",
    "        state_adj = (state - env.observation_space.low)*np.array([10, 100])\n",
    "        state_adj = np.round(state_adj, 0).astype(int)\n",
    "    \n",
    "        while done != True:   \n",
    "            # Render environment for last five episodes\n",
    "            if i >= (episodes - 20):\n",
    "                env.render()\n",
    "                \n",
    "            # Determine next action - epsilon greedy strategy\n",
    "            if np.random.random() < 1 - epsilon:\n",
    "                action = np.argmax(Q[state_adj[0], state_adj[1]]) \n",
    "            else:\n",
    "                action = np.random.randint(0, env.action_space.n)\n",
    "                \n",
    "            # Get next state and reward\n",
    "            state2, reward, done, info = env.step(action) \n",
    "            \n",
    "            # Discretize state2\n",
    "            state2_adj = (state2 - env.observation_space.low)*np.array([10, 100])\n",
    "            state2_adj = np.round(state2_adj, 0).astype(int)\n",
    "            \n",
    "            #Allow for terminal states\n",
    "            if done and state2[0] >= 0.5:\n",
    "                Q[state_adj[0], state_adj[1], action] = reward\n",
    "                \n",
    "            # Adjust Q value for current state\n",
    "            else:\n",
    "                delta = learning*(reward + \n",
    "                                 discount*np.max(Q[state2_adj[0], \n",
    "                                                   state2_adj[1]]) - \n",
    "                                 Q[state_adj[0], state_adj[1],action])\n",
    "                Q[state_adj[0], state_adj[1],action] += delta\n",
    "                                     \n",
    "            # Update variables\n",
    "            tot_reward += reward\n",
    "            state_adj = state2_adj\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if epsilon > min_eps:\n",
    "            epsilon -= reduction\n",
    "        \n",
    "        # Track rewards\n",
    "        reward_list.append(tot_reward)\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            ave_reward = np.mean(reward_list)\n",
    "            ave_reward_list.append(ave_reward)\n",
    "            reward_list = []\n",
    "            \n",
    "        if (i+1) % 100 == 0:    \n",
    "            print('Episode {} Average Reward: {}'.format(i+1, ave_reward))\n",
    "            \n",
    "    env.close()\n",
    "    \n",
    "    return ave_reward_list\n",
    "\n",
    "# Run Q-learning algorithm\n",
    "rewards = QLearning(env, 0.2, 0.9, 0.8, 0, 50000)\n",
    "\n",
    "# Plot Rewards\n",
    "plt.plot(100*(np.arange(len(rewards)) + 1), rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward vs Episodes')\n",
    "plt.savefig('rewards.jpg')     \n",
    "plt.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07276286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7302d996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56316fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae433fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1af41a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf77491a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c48e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a932013e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8463da4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a52f487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b157681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69d0f712",
   "metadata": {},
   "source": [
    "### CART POLE by Q-network\n",
    "https://skettee.github.io/post/q_network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12d81701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space:  Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Initial state:  [ 0.02811221 -0.0441772   0.00963665 -0.04237553]\n",
      "\n",
      "Action space:  Discrete(2)\n",
      "Random action:  1\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "#\n",
    "# Environment\n",
    "#\n",
    "env = gym.make('CartPole-v1')\n",
    "state = env.reset()\n",
    "action = env.action_space.sample()\n",
    "\n",
    "print('State space: ', env.observation_space)\n",
    "print('Initial state: ', state)\n",
    "print('\\nAction space: ', env.action_space)\n",
    "print('Random action: ', action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0344eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Network Modeling\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "num_state = env.observation_space.shape[0]\n",
    "num_action = env.action_space.n\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim= num_state, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_action, activation=None))\n",
    "model.compile(loss='mse', optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab40b063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▌                                                                            | 16/500 [07:43<5:03:42, 37.65s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_iteration = 500\n",
    "min_timesteps_per_batch = 2500\n",
    "\n",
    "# Hyper parameter\n",
    "epsilon = 0.3\n",
    "gamma = 0.95\n",
    "batch_size = 32\n",
    "\n",
    "# Q-Network Learning\n",
    "for i in tqdm(range(num_iteration)):\n",
    "    timesteps_this_batch = 0\n",
    "    memory = []\n",
    "    while True:\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.random.uniform() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                q_value = model.predict(state.reshape(1, num_state))\n",
    "                action = np.argmax(q_value[0])\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # Memory\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        timesteps_this_batch += len(memory)\n",
    "        if timesteps_this_batch > min_timesteps_per_batch:\n",
    "            break\n",
    "\n",
    "    # Replay   \n",
    "    for state, action, reward, next_state, done in memory:\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + gamma * (np.max(model.predict(next_state.reshape(1, num_state))[0]))\n",
    "        q_value = model.predict(state.reshape(1, num_state))\n",
    "        q_value[0][action] = target\n",
    "        model.fit(state.reshape(1, num_state), q_value, epochs=1, batch_size=32, verbose=0)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8e7a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_dir = os.getcwd()\n",
    "model_name = 'keras_dqn_trained_model.h5'\n",
    "\n",
    "# Save model and weights\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b3ef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "def show_video(file_infix):\n",
    "    with open('./video/openaigym.video.%s.video000000.mp4' % file_infix, 'r+b') as f:\n",
    "        video = f.read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"Trained CartPole\" autoplay \n",
    "                loop style=\"height: 200px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(encoded.decode('ascii'))))\n",
    "    \n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1625ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  tensorflow.keras.models import load_model\n",
    "import os\n",
    "\n",
    "load_dir = os.getcwd()\n",
    "model_name = 'keras_dqn_trained_model.h5'\n",
    "model_path = os.path.join(load_dir, model_name)\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f68b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = wrap_env(gym.make('CartPole-v1'))\n",
    "num_state = env.observation_space.shape[0]\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    state = np.array(state).reshape(1, num_state)\n",
    "    q_value = model.predict(state)\n",
    "    action = np.argmax(q_value[0])\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "file_infix = env.file_infix\n",
    "env.close()\n",
    "\n",
    "show_video(file_infix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43b6d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c96ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://jinicoding.net/openai-gym%EC%9C%BC%EB%A1%9C-q-table-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EB%A7%8C%EB%93%A4%EA%B8%B0-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78950f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 25000\n",
    "\n",
    "DISCRETE_OS_SIZE = [20, 20]\n",
    "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
    "\n",
    "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
    "\n",
    "\n",
    "def get_discrete_state(state):\n",
    "    discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
    "    return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table\n",
    "\n",
    "\n",
    "discrete_state = get_discrete_state(env.reset()) #초기 상태값을 가져옵니다.\n",
    "done = False\n",
    "while not done:\n",
    "\n",
    "    action = np.argmax(q_table[discrete_state]) #처음 예제에서 action = 2\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    new_discrete_state = get_discrete_state(new_state) #새로운 상태값\n",
    "\n",
    "    env.render()\n",
    "#     new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "#이제 Q-값을 업데이트합니다. 이미 만든 액션에 대한 Q값을 업데이트하고 있다는 점에 유의하십시오.\n",
    "    # If simulation did not end yet after last step - update Q table\n",
    "    if not done:\n",
    "\n",
    "        # Maximum possible Q value in next step (for new state)\n",
    "        max_future_q = np.max(q_table[new_discrete_state])\n",
    "\n",
    "        # Current Q value (for current state and performed action)\n",
    "        current_q = q_table[discrete_state + (action,)]\n",
    "\n",
    "        # And here's our equation for a new Q value for current state and action\n",
    "        new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "        # Update Q table with new Q value\n",
    "        q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "\n",
    "    # Simulation ended (for any reson) - if goal position is achived - update Q value with reward directly\n",
    "    elif new_state[0] >= env.goal_position:\n",
    "        #q_table[discrete_state + (action,)] = reward\n",
    "        q_table[discrete_state + (action,)] = 0\n",
    "\n",
    "    discrete_state = new_discrete_state\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da0100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2feb4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://incredible.ai/machine-learning/2019/02/10/Q-Learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1730fe4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: (19, 15, 3)\n",
      "Q Table\n",
      "[[[-0.02936094  0.74602922  0.33627678]\n",
      "  [-0.79968293 -0.5964087  -0.36720024]\n",
      "  [ 0.02152412  0.24354948  0.02032514]\n",
      "  [-0.795604   -0.72775485  0.40561727]\n",
      "  [-0.90822443 -0.784341   -0.27912673]\n",
      "  [-0.19860674  0.73301371 -0.60809526]\n",
      "  [ 0.87104498 -0.30647992 -0.44885352]\n",
      "  [-0.64489883 -0.55912948  0.0235889 ]\n",
      "  [-0.4630339  -0.38834287  0.41715608]\n",
      "  [-0.19330635  0.35999446 -0.21959744]\n",
      "  [ 0.20651598 -0.44123894 -0.10787226]\n",
      "  [-0.45748184  0.25516247 -0.34291327]\n",
      "  [ 0.27921404 -0.83795453  0.93058484]\n",
      "  [ 0.3807966  -0.26078929  0.72581876]\n",
      "  [-0.80068561 -0.49691774  0.75136827]]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.observation_space.high  # array([0.6 , 0.07], dtype=float32)\n",
    "env.observation_space.low   # array([-1.2 , -0.07], dtype=float32)\n",
    "env = gym.make('MountainCar-v0')\n",
    "n_state = (env.observation_space.high - env.observation_space.low) * np.array([10, 100])\n",
    "n_state = np.round(n_state, 0).astype(int) + 1\n",
    "\n",
    "Q = np.random.uniform(-1, 1, size=(n_state[0], n_state[1], env.action_space.n))\n",
    "print('Q shape:', Q.shape)\n",
    "print('Q Table')\n",
    "print(Q[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "053e16f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91d2c3145734a3c8e38e984e2e26cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 | tot reward:-200.0 | epsilon:0.8999 | rand action:178 | Q action:22\n",
      "epoch:100 | tot reward:-200.0 | epsilon:0.8899 | rand action:174 | Q action:26\n",
      "epoch:200 | tot reward:-200.0 | epsilon:0.8799 | rand action:172 | Q action:28\n",
      "epoch:300 | tot reward:-200.0 | epsilon:0.8699 | rand action:171 | Q action:29\n",
      "epoch:400 | tot reward:-200.0 | epsilon:0.8599 | rand action:167 | Q action:33\n",
      "epoch:500 | tot reward:-200.0 | epsilon:0.8499 | rand action:172 | Q action:28\n",
      "epoch:600 | tot reward:-200.0 | epsilon:0.8399 | rand action:168 | Q action:32\n",
      "epoch:700 | tot reward:-200.0 | epsilon:0.8299 | rand action:169 | Q action:31\n",
      "epoch:800 | tot reward:-200.0 | epsilon:0.8199 | rand action:172 | Q action:28\n",
      "epoch:900 | tot reward:-200.0 | epsilon:0.8099 | rand action:162 | Q action:38\n",
      "epoch:1000 | tot reward:-200.0 | epsilon:0.7999 | rand action:156 | Q action:44\n",
      "epoch:1100 | tot reward:-200.0 | epsilon:0.7899 | rand action:158 | Q action:42\n",
      "epoch:1200 | tot reward:-200.0 | epsilon:0.7799 | rand action:146 | Q action:54\n",
      "epoch:1300 | tot reward:-200.0 | epsilon:0.7699 | rand action:157 | Q action:43\n",
      "epoch:1400 | tot reward:-200.0 | epsilon:0.7599 | rand action:138 | Q action:62\n",
      "epoch:1500 | tot reward:-200.0 | epsilon:0.7499 | rand action:137 | Q action:63\n",
      "epoch:1600 | tot reward:-200.0 | epsilon:0.7399 | rand action:150 | Q action:50\n",
      "epoch:1700 | tot reward:-200.0 | epsilon:0.7299 | rand action:143 | Q action:57\n",
      "epoch:1800 | tot reward:-200.0 | epsilon:0.7199 | rand action:141 | Q action:59\n",
      "epoch:1900 | tot reward:-200.0 | epsilon:0.7099 | rand action:150 | Q action:50\n",
      "epoch:2000 | tot reward:-200.0 | epsilon:0.6999 | rand action:143 | Q action:57\n",
      "epoch:2100 | tot reward:-200.0 | epsilon:0.6899 | rand action:143 | Q action:57\n",
      "epoch:2200 | tot reward:-200.0 | epsilon:0.6799 | rand action:136 | Q action:64\n",
      "epoch:2300 | tot reward:-200.0 | epsilon:0.6699 | rand action:136 | Q action:64\n",
      "epoch:2400 | tot reward:-200.0 | epsilon:0.6599 | rand action:134 | Q action:66\n",
      "epoch:2500 | tot reward:-200.0 | epsilon:0.6499 | rand action:140 | Q action:60\n",
      "epoch:2600 | tot reward:-200.0 | epsilon:0.6399 | rand action:135 | Q action:65\n",
      "epoch:2700 | tot reward:-200.0 | epsilon:0.6299 | rand action:133 | Q action:67\n",
      "epoch:2800 | tot reward:-200.0 | epsilon:0.6199 | rand action:130 | Q action:70\n",
      "epoch:2900 | tot reward:-200.0 | epsilon:0.6099 | rand action:116 | Q action:84\n",
      "epoch:3000 | tot reward:-200.0 | epsilon:0.5999 | rand action:128 | Q action:72\n",
      "epoch:3100 | tot reward:-200.0 | epsilon:0.5899 | rand action:111 | Q action:89\n",
      "epoch:3200 | tot reward:-200.0 | epsilon:0.5799 | rand action:134 | Q action:66\n",
      "epoch:3300 | tot reward:-200.0 | epsilon:0.5699 | rand action:121 | Q action:79\n",
      "epoch:3400 | tot reward:-200.0 | epsilon:0.5599 | rand action:105 | Q action:95\n",
      "epoch:3500 | tot reward:-200.0 | epsilon:0.5499 | rand action:111 | Q action:89\n",
      "epoch:3600 | tot reward:-200.0 | epsilon:0.5399 | rand action:117 | Q action:83\n",
      "epoch:3700 | tot reward:-200.0 | epsilon:0.5299 | rand action:110 | Q action:90\n",
      "epoch:3800 | tot reward:-200.0 | epsilon:0.5199 | rand action:115 | Q action:85\n",
      "epoch:3900 | tot reward:-200.0 | epsilon:0.5099 | rand action:109 | Q action:91\n",
      "epoch:4000 | tot reward:-200.0 | epsilon:0.4999 | rand action:92 | Q action:108\n",
      "epoch:4100 | tot reward:-200.0 | epsilon:0.4899 | rand action:108 | Q action:92\n",
      "epoch:4200 | tot reward:-200.0 | epsilon:0.4799 | rand action:93 | Q action:107\n",
      "epoch:4300 | tot reward:-200.0 | epsilon:0.4699 | rand action:100 | Q action:100\n",
      "epoch:4400 | tot reward:-200.0 | epsilon:0.4599 | rand action:89 | Q action:111\n",
      "epoch:4500 | tot reward:-200.0 | epsilon:0.4499 | rand action:85 | Q action:115\n",
      "epoch:4600 | tot reward:-200.0 | epsilon:0.4399 | rand action:88 | Q action:112\n",
      "epoch:4700 | tot reward:-200.0 | epsilon:0.4299 | rand action:83 | Q action:117\n",
      "epoch:4800 | tot reward:-200.0 | epsilon:0.4199 | rand action:80 | Q action:120\n",
      "epoch:4900 | tot reward:-200.0 | epsilon:0.4099 | rand action:85 | Q action:115\n",
      "epoch:5000 | tot reward:-200.0 | epsilon:0.3999 | rand action:77 | Q action:123\n",
      "epoch:5100 | tot reward:-200.0 | epsilon:0.3899 | rand action:81 | Q action:119\n",
      "epoch:5200 | tot reward:-200.0 | epsilon:0.3799 | rand action:75 | Q action:125\n",
      "epoch:5300 | tot reward:-200.0 | epsilon:0.3699 | rand action:66 | Q action:134\n",
      "epoch:5400 | tot reward:-200.0 | epsilon:0.3599 | rand action:67 | Q action:133\n",
      "epoch:5500 | tot reward:-200.0 | epsilon:0.3499 | rand action:68 | Q action:132\n",
      "epoch:5600 | tot reward:-200.0 | epsilon:0.3399 | rand action:67 | Q action:133\n",
      "epoch:5700 | tot reward:-200.0 | epsilon:0.3299 | rand action:69 | Q action:131\n",
      "epoch:5800 | tot reward:-200.0 | epsilon:0.3199 | rand action:60 | Q action:140\n",
      "epoch:5900 | tot reward:-200.0 | epsilon:0.3099 | rand action:80 | Q action:120\n",
      "epoch:6000 | tot reward:-200.0 | epsilon:0.2999 | rand action:63 | Q action:137\n",
      "epoch:6100 | tot reward:-200.0 | epsilon:0.2899 | rand action:58 | Q action:142\n",
      "epoch:6200 | tot reward:-200.0 | epsilon:0.2799 | rand action:49 | Q action:151\n",
      "epoch:6300 | tot reward:-200.0 | epsilon:0.2699 | rand action:53 | Q action:147\n",
      "epoch:6400 | tot reward:-200.0 | epsilon:0.2599 | rand action:56 | Q action:144\n",
      "epoch:6500 | tot reward:-200.0 | epsilon:0.2499 | rand action:39 | Q action:161\n",
      "epoch:6600 | tot reward:-170.0 | epsilon:0.2399 | rand action:40 | Q action:130\n",
      "epoch:6700 | tot reward:-200.0 | epsilon:0.2299 | rand action:62 | Q action:138\n",
      "epoch:6800 | tot reward:-200.0 | epsilon:0.2199 | rand action:51 | Q action:149\n",
      "epoch:6900 | tot reward:-200.0 | epsilon:0.2099 | rand action:48 | Q action:152\n",
      "epoch:7000 | tot reward:-200.0 | epsilon:0.1999 | rand action:33 | Q action:167\n",
      "epoch:7100 | tot reward:-200.0 | epsilon:0.1899 | rand action:39 | Q action:161\n",
      "epoch:7200 | tot reward:-200.0 | epsilon:0.1799 | rand action:32 | Q action:168\n",
      "epoch:7300 | tot reward:-200.0 | epsilon:0.1699 | rand action:35 | Q action:165\n",
      "epoch:7400 | tot reward:-200.0 | epsilon:0.1599 | rand action:35 | Q action:165\n",
      "epoch:7500 | tot reward:-173.0 | epsilon:0.1499 | rand action:28 | Q action:145\n",
      "epoch:7600 | tot reward:-200.0 | epsilon:0.1399 | rand action:35 | Q action:165\n",
      "epoch:7700 | tot reward:-200.0 | epsilon:0.1299 | rand action:31 | Q action:169\n",
      "epoch:7800 | tot reward:-200.0 | epsilon:0.1199 | rand action:33 | Q action:167\n",
      "epoch:7900 | tot reward:-200.0 | epsilon:0.1099 | rand action:20 | Q action:180\n",
      "epoch:8000 | tot reward:-200.0 | epsilon:0.0999 | rand action:25 | Q action:175\n",
      "epoch:8100 | tot reward:-200.0 | epsilon:0.0899 | rand action:17 | Q action:183\n",
      "epoch:8200 | tot reward:-169.0 | epsilon:0.0799 | rand action:17 | Q action:152\n",
      "epoch:8300 | tot reward:-154.0 | epsilon:0.0699 | rand action:14 | Q action:140\n",
      "epoch:8400 | tot reward:-200.0 | epsilon:0.0599 | rand action:13 | Q action:187\n",
      "epoch:8500 | tot reward:-200.0 | epsilon:0.0499 | rand action:7 | Q action:193\n",
      "epoch:8600 | tot reward:-200.0 | epsilon:0.0399 | rand action:10 | Q action:190\n",
      "epoch:8700 | tot reward:-200.0 | epsilon:0.0299 | rand action:12 | Q action:188\n",
      "epoch:8800 | tot reward:-172.0 | epsilon:0.0199 | rand action:3 | Q action:169\n",
      "epoch:8900 | tot reward:-200.0 | epsilon:0.0099 | rand action:1 | Q action:199\n",
      "epoch:9000 | tot reward:-156.0 | epsilon:0.0 | rand action:0 | Q action:156\n",
      "epoch:9100 | tot reward:-164.0 | epsilon:0.0 | rand action:0 | Q action:164\n",
      "epoch:9200 | tot reward:-161.0 | epsilon:0.0 | rand action:0 | Q action:161\n",
      "epoch:9300 | tot reward:-200.0 | epsilon:0.0 | rand action:0 | Q action:200\n",
      "epoch:9400 | tot reward:-156.0 | epsilon:0.0 | rand action:0 | Q action:156\n",
      "epoch:9500 | tot reward:-154.0 | epsilon:0.0 | rand action:0 | Q action:154\n",
      "epoch:9600 | tot reward:-146.0 | epsilon:0.0 | rand action:0 | Q action:146\n",
      "epoch:9700 | tot reward:-144.0 | epsilon:0.0 | rand action:0 | Q action:144\n",
      "epoch:9800 | tot reward:-200.0 | epsilon:0.0 | rand action:0 | Q action:200\n",
      "epoch:9900 | tot reward:-152.0 | epsilon:0.0 | rand action:0 | Q action:152\n"
     ]
    }
   ],
   "source": [
    "from tqdm import *\n",
    "def discretize(env, state):\n",
    "    state = (state - env.observation_space.low) * np.array([10, 100])\n",
    "    state = np.round(state, 0).astype(int)\n",
    "    return state\n",
    "\n",
    "def train(env, Q, epochs=10000, lr=0.1, gamma=0.9, epsilon=0.9):\n",
    "    reduction = epsilon/epochs\n",
    "    action_n = env.action_space.n\n",
    "    \n",
    "    rewards = list()\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(epochs)):\n",
    "        state = env.reset()\n",
    "        state = discretize(env, state)\n",
    "        \n",
    "        done = False\n",
    "        _tot_reward = 0\n",
    "        _tot_rand_action = 0\n",
    "        _tot_q_action = 0\n",
    "        _max_pos = 0\n",
    "        \n",
    "        while not done:\n",
    "\n",
    "            # Calculate next action\n",
    "            if np.random.random() < 1 - epsilon:\n",
    "                action = np.argmax(Q[state[0], state[1]])\n",
    "                _tot_q_action += 1\n",
    "            else:\n",
    "                action = np.random.randint(0, action_n)\n",
    "                _tot_rand_action += 1\n",
    "                \n",
    "            # Step!\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state_apx = discretize(env, next_state)\n",
    "\n",
    "            # Terminal Update\n",
    "            if done and next_state[0] >= 0.5:\n",
    "                Q[next_state_apx[0], next_state_apx[1], action] = reward\n",
    "            else:\n",
    "                delta = lr * (reward + gamma * np.max(Q[next_state_apx[0], next_state_apx[1]]) - \n",
    "                              Q[state[0], state[1], action])\n",
    "                Q[state[0], state[1], action] += delta\n",
    "            \n",
    "            state = next_state_apx\n",
    "            _tot_reward += reward\n",
    "            \n",
    "        # Decay Epsilon\n",
    "        if epsilon > 0:\n",
    "            epsilon -= reduction\n",
    "            epsilon = round(epsilon, 4)\n",
    "            \n",
    "        # Track Rewards\n",
    "        rewards.append(_tot_reward)\n",
    "        \n",
    "        # Log\n",
    "        if epoch%100 == 0:\n",
    "            print(f'\\repoch:{epoch} | tot reward:{_tot_reward} | epsilon:{epsilon} | ' \n",
    "                  f'rand action:{_tot_rand_action} | Q action:{_tot_q_action}')\n",
    "\n",
    "train(env, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "322dd970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:[7 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 7 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 8 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[8 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[8 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[8 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[9 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[9 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[9 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[9 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[9 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[10  9] | reward:-1.0 | done:False | info:{}\n",
      "state:[10  9] | reward:-1.0 | done:False | info:{}\n",
      "state:[10  9] | reward:-1.0 | done:False | info:{}\n",
      "state:[10  9] | reward:-1.0 | done:False | info:{}\n",
      "state:[10  9] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  9] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  9] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  8] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  8] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  8] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  7] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  7] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  7] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  6] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  6] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  6] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  5] | reward:-1.0 | done:False | info:{}\n",
      "state:[10  5] | reward:-1.0 | done:False | info:{}\n",
      "state:[10  5] | reward:-1.0 | done:False | info:{}\n",
      "state:[10  4] | reward:-1.0 | done:False | info:{}\n",
      "state:[10  4] | reward:-1.0 | done:False | info:{}\n",
      "state:[9 4] | reward:-1.0 | done:False | info:{}\n",
      "state:[9 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[9 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[8 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[8 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 4] | reward:-1.0 | done:False | info:{}\n",
      "state:[3 4] | reward:-1.0 | done:False | info:{}\n",
      "state:[3 4] | reward:-1.0 | done:False | info:{}\n",
      "state:[3 4] | reward:-1.0 | done:False | info:{}\n",
      "state:[3 5] | reward:-1.0 | done:False | info:{}\n",
      "state:[2 5] | reward:-1.0 | done:False | info:{}\n",
      "state:[2 5] | reward:-1.0 | done:False | info:{}\n",
      "state:[2 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[2 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[2 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[2 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[2 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[2 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[2 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[2 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[2 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[2 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[2 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[2 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[3 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 3 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 3 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 4 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 4 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 4 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 5 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 5 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 6 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 6 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 6 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 7 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 7 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 8 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 8 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 9 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 9 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[10 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[10 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[11 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[11 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[12 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[12 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[12 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[13 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[13 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[13 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[14 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[14 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[14 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[15 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[15 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[15 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[15 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[16 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[16 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[16 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[16 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[17 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[17 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[17  9] | reward:-1.0 | done:True | info:{}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "state = env.reset()\n",
    "state = discretize(env, state)\n",
    "\n",
    "\n",
    "# input()\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    action = np.argmax(Q[state[0], state[1]])\n",
    "    state, reward, done, info = env.step(action)\n",
    "    state = discretize(env, state)\n",
    "    \n",
    "    print(f'\\rstate:{state} | reward:{reward} | done:{done} | info:{info}')\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac7e200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bdd598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070d8b79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c452f427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c00e9bf7",
   "metadata": {},
   "source": [
    "## coursehero 참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14354118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  https://www.coursehero.com/u/file/119544747/Mountain-Carpy/?justUnlocked=1#question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ba4987d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 Average Reward: -200.0\n",
      "Episode 200 Average Reward: -200.0\n",
      "Episode 300 Average Reward: -200.0\n",
      "Episode 400 Average Reward: -200.0\n",
      "Episode 500 Average Reward: -200.0\n",
      "Episode 600 Average Reward: -200.0\n",
      "Episode 700 Average Reward: -200.0\n",
      "Episode 800 Average Reward: -200.0\n",
      "Episode 900 Average Reward: -200.0\n",
      "Episode 1000 Average Reward: -200.0\n",
      "Episode 1100 Average Reward: -200.0\n",
      "Episode 1200 Average Reward: -200.0\n",
      "Episode 1300 Average Reward: -200.0\n",
      "Episode 1400 Average Reward: -200.0\n",
      "Episode 1500 Average Reward: -200.0\n",
      "Episode 1600 Average Reward: -200.0\n",
      "Episode 1700 Average Reward: -200.0\n",
      "Episode 1800 Average Reward: -200.0\n",
      "Episode 1900 Average Reward: -200.0\n",
      "Episode 2000 Average Reward: -200.0\n",
      "Episode 2100 Average Reward: -200.0\n",
      "Episode 2200 Average Reward: -200.0\n",
      "Episode 2300 Average Reward: -200.0\n",
      "Episode 2400 Average Reward: -200.0\n",
      "Episode 2500 Average Reward: -200.0\n",
      "Episode 2600 Average Reward: -200.0\n",
      "Episode 2700 Average Reward: -200.0\n",
      "Episode 2800 Average Reward: -200.0\n",
      "Episode 2900 Average Reward: -200.0\n",
      "Episode 3000 Average Reward: -200.0\n",
      "Episode 3100 Average Reward: -200.0\n",
      "Episode 3200 Average Reward: -200.0\n",
      "Episode 3300 Average Reward: -200.0\n",
      "Episode 3400 Average Reward: -200.0\n",
      "Episode 3500 Average Reward: -200.0\n",
      "Episode 3600 Average Reward: -200.0\n",
      "Episode 3700 Average Reward: -200.0\n",
      "Episode 3800 Average Reward: -200.0\n",
      "Episode 3900 Average Reward: -200.0\n",
      "Episode 4000 Average Reward: -200.0\n",
      "Episode 4100 Average Reward: -200.0\n",
      "Episode 4200 Average Reward: -200.0\n",
      "Episode 4300 Average Reward: -200.0\n",
      "Episode 4400 Average Reward: -200.0\n",
      "Episode 4500 Average Reward: -200.0\n",
      "Episode 4600 Average Reward: -200.0\n",
      "Episode 4700 Average Reward: -200.0\n",
      "Episode 4800 Average Reward: -200.0\n",
      "Episode 4900 Average Reward: -200.0\n",
      "Episode 5000 Average Reward: -200.0\n",
      "Episode 5100 Average Reward: -200.0\n",
      "Episode 5200 Average Reward: -200.0\n",
      "Episode 5300 Average Reward: -200.0\n",
      "Episode 5400 Average Reward: -200.0\n",
      "Episode 5500 Average Reward: -200.0\n",
      "Episode 5600 Average Reward: -200.0\n",
      "Episode 5700 Average Reward: -200.0\n",
      "Episode 5800 Average Reward: -200.0\n",
      "Episode 5900 Average Reward: -200.0\n",
      "Episode 6000 Average Reward: -200.0\n",
      "Episode 6100 Average Reward: -200.0\n",
      "Episode 6200 Average Reward: -200.0\n",
      "Episode 6300 Average Reward: -200.0\n",
      "Episode 6400 Average Reward: -200.0\n",
      "Episode 6500 Average Reward: -200.0\n",
      "Episode 6600 Average Reward: -200.0\n",
      "Episode 6700 Average Reward: -200.0\n",
      "Episode 6800 Average Reward: -200.0\n",
      "Episode 6900 Average Reward: -200.0\n",
      "Episode 7000 Average Reward: -200.0\n",
      "Episode 7100 Average Reward: -200.0\n",
      "Episode 7200 Average Reward: -200.0\n",
      "Episode 7300 Average Reward: -200.0\n",
      "Episode 7400 Average Reward: -200.0\n",
      "Episode 7500 Average Reward: -200.0\n",
      "Episode 7600 Average Reward: -200.0\n",
      "Episode 7700 Average Reward: -200.0\n",
      "Episode 7800 Average Reward: -200.0\n",
      "Episode 7900 Average Reward: -200.0\n",
      "Episode 8000 Average Reward: -200.0\n",
      "Episode 8100 Average Reward: -200.0\n",
      "Episode 8200 Average Reward: -200.0\n",
      "Episode 8300 Average Reward: -200.0\n",
      "Episode 8400 Average Reward: -200.0\n",
      "Episode 8500 Average Reward: -200.0\n",
      "Episode 8600 Average Reward: -200.0\n",
      "Episode 8700 Average Reward: -200.0\n",
      "Episode 8800 Average Reward: -200.0\n",
      "Episode 8900 Average Reward: -200.0\n",
      "Episode 9000 Average Reward: -200.0\n",
      "Episode 9100 Average Reward: -200.0\n",
      "Episode 9200 Average Reward: -200.0\n",
      "Episode 9300 Average Reward: -200.0\n",
      "Episode 9400 Average Reward: -200.0\n",
      "Episode 9500 Average Reward: -200.0\n",
      "Episode 9600 Average Reward: -200.0\n",
      "Episode 9700 Average Reward: -200.0\n",
      "Episode 9800 Average Reward: -200.0\n",
      "Episode 9900 Average Reward: -200.0\n",
      "Episode 10000 Average Reward: -200.0\n",
      "Episode 10100 Average Reward: -200.0\n",
      "Episode 10200 Average Reward: -200.0\n",
      "Episode 10300 Average Reward: -200.0\n",
      "Episode 10400 Average Reward: -200.0\n",
      "Episode 10500 Average Reward: -200.0\n",
      "Episode 10600 Average Reward: -200.0\n",
      "Episode 10700 Average Reward: -200.0\n",
      "Episode 10800 Average Reward: -200.0\n",
      "Episode 10900 Average Reward: -200.0\n",
      "Episode 11000 Average Reward: -200.0\n",
      "Episode 11100 Average Reward: -200.0\n",
      "Episode 11200 Average Reward: -200.0\n",
      "Episode 11300 Average Reward: -200.0\n",
      "Episode 11400 Average Reward: -200.0\n",
      "Episode 11500 Average Reward: -200.0\n",
      "Episode 11600 Average Reward: -200.0\n",
      "Episode 11700 Average Reward: -200.0\n",
      "Episode 11800 Average Reward: -200.0\n",
      "Episode 11900 Average Reward: -200.0\n",
      "Episode 12000 Average Reward: -200.0\n",
      "Episode 12100 Average Reward: -200.0\n",
      "Episode 12200 Average Reward: -200.0\n",
      "Episode 12300 Average Reward: -200.0\n",
      "Episode 12400 Average Reward: -200.0\n",
      "Episode 12500 Average Reward: -200.0\n",
      "Episode 12600 Average Reward: -200.0\n",
      "Episode 12700 Average Reward: -200.0\n",
      "Episode 12800 Average Reward: -200.0\n",
      "Episode 12900 Average Reward: -200.0\n",
      "Episode 13000 Average Reward: -200.0\n",
      "Episode 13100 Average Reward: -200.0\n",
      "Episode 13200 Average Reward: -200.0\n",
      "Episode 13300 Average Reward: -200.0\n",
      "Episode 13400 Average Reward: -200.0\n",
      "Episode 13500 Average Reward: -200.0\n",
      "Episode 13600 Average Reward: -200.0\n",
      "Episode 13700 Average Reward: -200.0\n",
      "Episode 13800 Average Reward: -200.0\n",
      "Episode 13900 Average Reward: -200.0\n",
      "Episode 14000 Average Reward: -200.0\n",
      "Episode 14100 Average Reward: -200.0\n",
      "Episode 14200 Average Reward: -200.0\n",
      "Episode 14300 Average Reward: -200.0\n",
      "Episode 14400 Average Reward: -200.0\n",
      "Episode 14500 Average Reward: -200.0\n",
      "Episode 14600 Average Reward: -200.0\n",
      "Episode 14700 Average Reward: -200.0\n",
      "Episode 14800 Average Reward: -200.0\n",
      "Episode 14900 Average Reward: -200.0\n",
      "Episode 15000 Average Reward: -200.0\n",
      "Episode 15100 Average Reward: -200.0\n",
      "Episode 15200 Average Reward: -200.0\n",
      "Episode 15300 Average Reward: -200.0\n",
      "Episode 15400 Average Reward: -200.0\n",
      "Episode 15500 Average Reward: -200.0\n",
      "Episode 15600 Average Reward: -200.0\n",
      "Episode 15700 Average Reward: -200.0\n",
      "Episode 15800 Average Reward: -200.0\n",
      "Episode 15900 Average Reward: -200.0\n",
      "Episode 16000 Average Reward: -200.0\n",
      "Episode 16100 Average Reward: -200.0\n",
      "Episode 16200 Average Reward: -200.0\n",
      "Episode 16300 Average Reward: -200.0\n",
      "Episode 16400 Average Reward: -200.0\n",
      "Episode 16500 Average Reward: -200.0\n",
      "Episode 16600 Average Reward: -200.0\n",
      "Episode 16700 Average Reward: -200.0\n",
      "Episode 16800 Average Reward: -200.0\n",
      "Episode 16900 Average Reward: -200.0\n",
      "Episode 17000 Average Reward: -200.0\n",
      "Episode 17100 Average Reward: -200.0\n",
      "Episode 17200 Average Reward: -200.0\n",
      "Episode 17300 Average Reward: -200.0\n",
      "Episode 17400 Average Reward: -200.0\n",
      "Episode 17500 Average Reward: -200.0\n",
      "Episode 17600 Average Reward: -200.0\n",
      "Episode 17700 Average Reward: -200.0\n",
      "Episode 17800 Average Reward: -200.0\n",
      "Episode 17900 Average Reward: -200.0\n",
      "Episode 18000 Average Reward: -200.0\n",
      "Episode 18100 Average Reward: -200.0\n",
      "Episode 18200 Average Reward: -200.0\n",
      "Episode 18300 Average Reward: -200.0\n",
      "Episode 18400 Average Reward: -200.0\n",
      "Episode 18500 Average Reward: -200.0\n",
      "Episode 18600 Average Reward: -200.0\n",
      "Episode 18700 Average Reward: -200.0\n",
      "Episode 18800 Average Reward: -200.0\n",
      "Episode 18900 Average Reward: -200.0\n",
      "Episode 19000 Average Reward: -200.0\n",
      "Episode 19100 Average Reward: -200.0\n",
      "Episode 19200 Average Reward: -200.0\n",
      "Episode 19300 Average Reward: -200.0\n",
      "Episode 19400 Average Reward: -200.0\n",
      "Episode 19500 Average Reward: -200.0\n",
      "Episode 19600 Average Reward: -200.0\n",
      "Episode 19700 Average Reward: -200.0\n",
      "Episode 19800 Average Reward: -200.0\n",
      "Episode 19900 Average Reward: -199.67\n",
      "Episode 20000 Average Reward: -200.0\n",
      "Episode 20100 Average Reward: -200.0\n",
      "Episode 20200 Average Reward: -200.0\n",
      "Episode 20300 Average Reward: -200.0\n",
      "Episode 20400 Average Reward: -200.0\n",
      "Episode 20500 Average Reward: -200.0\n",
      "Episode 20600 Average Reward: -200.0\n",
      "Episode 20700 Average Reward: -200.0\n",
      "Episode 20800 Average Reward: -200.0\n",
      "Episode 20900 Average Reward: -200.0\n",
      "Episode 21000 Average Reward: -200.0\n",
      "Episode 21100 Average Reward: -200.0\n",
      "Episode 21200 Average Reward: -200.0\n",
      "Episode 21300 Average Reward: -200.0\n",
      "Episode 21400 Average Reward: -200.0\n",
      "Episode 21500 Average Reward: -200.0\n",
      "Episode 21600 Average Reward: -199.69\n",
      "Episode 21700 Average Reward: -200.0\n",
      "Episode 21800 Average Reward: -200.0\n",
      "Episode 21900 Average Reward: -200.0\n",
      "Episode 22000 Average Reward: -200.0\n",
      "Episode 22100 Average Reward: -200.0\n",
      "Episode 22200 Average Reward: -200.0\n",
      "Episode 22300 Average Reward: -200.0\n",
      "Episode 22400 Average Reward: -200.0\n",
      "Episode 22500 Average Reward: -199.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 22600 Average Reward: -199.69\n",
      "Episode 22700 Average Reward: -200.0\n",
      "Episode 22800 Average Reward: -200.0\n",
      "Episode 22900 Average Reward: -200.0\n",
      "Episode 23000 Average Reward: -200.0\n",
      "Episode 23100 Average Reward: -200.0\n",
      "Episode 23200 Average Reward: -200.0\n",
      "Episode 23300 Average Reward: -200.0\n",
      "Episode 23400 Average Reward: -200.0\n",
      "Episode 23500 Average Reward: -200.0\n",
      "Episode 23600 Average Reward: -200.0\n",
      "Episode 23700 Average Reward: -200.0\n",
      "Episode 23800 Average Reward: -200.0\n",
      "Episode 23900 Average Reward: -200.0\n",
      "Episode 24000 Average Reward: -199.61\n",
      "Episode 24100 Average Reward: -200.0\n",
      "Episode 24200 Average Reward: -200.0\n",
      "Episode 24300 Average Reward: -200.0\n",
      "Episode 24400 Average Reward: -200.0\n",
      "Episode 24500 Average Reward: -200.0\n",
      "Episode 24600 Average Reward: -200.0\n",
      "Episode 24700 Average Reward: -200.0\n",
      "Episode 24800 Average Reward: -200.0\n",
      "Episode 24900 Average Reward: -200.0\n",
      "Episode 25000 Average Reward: -200.0\n",
      "Episode 25100 Average Reward: -200.0\n",
      "Episode 25200 Average Reward: -200.0\n",
      "Episode 25300 Average Reward: -200.0\n",
      "Episode 25400 Average Reward: -200.0\n",
      "Episode 25500 Average Reward: -200.0\n",
      "Episode 25600 Average Reward: -200.0\n",
      "Episode 25700 Average Reward: -200.0\n",
      "Episode 25800 Average Reward: -200.0\n",
      "Episode 25900 Average Reward: -200.0\n",
      "Episode 26000 Average Reward: -200.0\n",
      "Episode 26100 Average Reward: -200.0\n",
      "Episode 26200 Average Reward: -200.0\n",
      "Episode 26300 Average Reward: -200.0\n",
      "Episode 26400 Average Reward: -200.0\n",
      "Episode 26500 Average Reward: -200.0\n",
      "Episode 26600 Average Reward: -200.0\n",
      "Episode 26700 Average Reward: -200.0\n",
      "Episode 26800 Average Reward: -200.0\n",
      "Episode 26900 Average Reward: -199.58\n",
      "Episode 27000 Average Reward: -199.92\n",
      "Episode 27100 Average Reward: -198.79\n",
      "Episode 27200 Average Reward: -199.98\n",
      "Episode 27300 Average Reward: -199.61\n",
      "Episode 27400 Average Reward: -200.0\n",
      "Episode 27500 Average Reward: -200.0\n",
      "Episode 27600 Average Reward: -200.0\n",
      "Episode 27700 Average Reward: -200.0\n",
      "Episode 27800 Average Reward: -200.0\n",
      "Episode 27900 Average Reward: -200.0\n",
      "Episode 28000 Average Reward: -199.98\n",
      "Episode 28100 Average Reward: -200.0\n",
      "Episode 28200 Average Reward: -199.41\n",
      "Episode 28300 Average Reward: -199.61\n",
      "Episode 28400 Average Reward: -200.0\n",
      "Episode 28500 Average Reward: -199.83\n",
      "Episode 28600 Average Reward: -199.99\n",
      "Episode 28700 Average Reward: -199.59\n",
      "Episode 28800 Average Reward: -200.0\n",
      "Episode 28900 Average Reward: -200.0\n",
      "Episode 29000 Average Reward: -200.0\n",
      "Episode 29100 Average Reward: -200.0\n",
      "Episode 29200 Average Reward: -198.6\n",
      "Episode 29300 Average Reward: -199.98\n",
      "Episode 29400 Average Reward: -200.0\n",
      "Episode 29500 Average Reward: -200.0\n",
      "Episode 29600 Average Reward: -200.0\n",
      "Episode 29700 Average Reward: -200.0\n",
      "Episode 29800 Average Reward: -199.94\n",
      "Episode 29900 Average Reward: -200.0\n",
      "Episode 30000 Average Reward: -200.0\n",
      "Episode 30100 Average Reward: -199.74\n",
      "Episode 30200 Average Reward: -199.73\n",
      "Episode 30300 Average Reward: -199.45\n",
      "Episode 30400 Average Reward: -199.2\n",
      "Episode 30500 Average Reward: -199.65\n",
      "Episode 30600 Average Reward: -200.0\n",
      "Episode 30700 Average Reward: -200.0\n",
      "Episode 30800 Average Reward: -200.0\n",
      "Episode 30900 Average Reward: -199.61\n",
      "Episode 31000 Average Reward: -199.64\n",
      "Episode 31100 Average Reward: -200.0\n",
      "Episode 31200 Average Reward: -200.0\n",
      "Episode 31300 Average Reward: -199.29\n",
      "Episode 31400 Average Reward: -198.48\n",
      "Episode 31500 Average Reward: -199.53\n",
      "Episode 31600 Average Reward: -200.0\n",
      "Episode 31700 Average Reward: -200.0\n",
      "Episode 31800 Average Reward: -200.0\n",
      "Episode 31900 Average Reward: -199.61\n",
      "Episode 32000 Average Reward: -198.59\n",
      "Episode 32100 Average Reward: -198.32\n",
      "Episode 32200 Average Reward: -198.7\n",
      "Episode 32300 Average Reward: -200.0\n",
      "Episode 32400 Average Reward: -200.0\n",
      "Episode 32500 Average Reward: -199.8\n",
      "Episode 32600 Average Reward: -200.0\n",
      "Episode 32700 Average Reward: -200.0\n",
      "Episode 32800 Average Reward: -199.54\n",
      "Episode 32900 Average Reward: -199.89\n",
      "Episode 33000 Average Reward: -199.44\n",
      "Episode 33100 Average Reward: -199.66\n",
      "Episode 33200 Average Reward: -198.64\n",
      "Episode 33300 Average Reward: -198.87\n",
      "Episode 33400 Average Reward: -199.2\n",
      "Episode 33500 Average Reward: -199.59\n",
      "Episode 33600 Average Reward: -200.0\n",
      "Episode 33700 Average Reward: -199.74\n",
      "Episode 33800 Average Reward: -198.87\n",
      "Episode 33900 Average Reward: -198.99\n",
      "Episode 34000 Average Reward: -199.46\n",
      "Episode 34100 Average Reward: -198.08\n",
      "Episode 34200 Average Reward: -199.51\n",
      "Episode 34300 Average Reward: -198.24\n",
      "Episode 34400 Average Reward: -198.22\n",
      "Episode 34500 Average Reward: -199.04\n",
      "Episode 34600 Average Reward: -197.75\n",
      "Episode 34700 Average Reward: -197.34\n",
      "Episode 34800 Average Reward: -198.66\n",
      "Episode 34900 Average Reward: -197.54\n",
      "Episode 35000 Average Reward: -199.22\n",
      "Episode 35100 Average Reward: -198.0\n",
      "Episode 35200 Average Reward: -198.27\n",
      "Episode 35300 Average Reward: -198.88\n",
      "Episode 35400 Average Reward: -200.0\n",
      "Episode 35500 Average Reward: -200.0\n",
      "Episode 35600 Average Reward: -200.0\n",
      "Episode 35700 Average Reward: -197.86\n",
      "Episode 35800 Average Reward: -197.28\n",
      "Episode 35900 Average Reward: -198.85\n",
      "Episode 36000 Average Reward: -199.0\n",
      "Episode 36100 Average Reward: -199.54\n",
      "Episode 36200 Average Reward: -199.02\n",
      "Episode 36300 Average Reward: -198.62\n",
      "Episode 36400 Average Reward: -196.56\n",
      "Episode 36500 Average Reward: -196.4\n",
      "Episode 36600 Average Reward: -198.58\n",
      "Episode 36700 Average Reward: -199.63\n",
      "Episode 36800 Average Reward: -199.06\n",
      "Episode 36900 Average Reward: -197.49\n",
      "Episode 37000 Average Reward: -198.61\n",
      "Episode 37100 Average Reward: -199.89\n",
      "Episode 37200 Average Reward: -199.21\n",
      "Episode 37300 Average Reward: -198.45\n",
      "Episode 37400 Average Reward: -198.06\n",
      "Episode 37500 Average Reward: -198.61\n",
      "Episode 37600 Average Reward: -199.88\n",
      "Episode 37700 Average Reward: -200.0\n",
      "Episode 37800 Average Reward: -199.4\n",
      "Episode 37900 Average Reward: -197.04\n",
      "Episode 38000 Average Reward: -198.17\n",
      "Episode 38100 Average Reward: -196.93\n",
      "Episode 38200 Average Reward: -199.24\n",
      "Episode 38300 Average Reward: -199.57\n",
      "Episode 38400 Average Reward: -199.92\n",
      "Episode 38500 Average Reward: -199.17\n",
      "Episode 38600 Average Reward: -199.11\n",
      "Episode 38700 Average Reward: -197.16\n",
      "Episode 38800 Average Reward: -198.59\n",
      "Episode 38900 Average Reward: -197.9\n",
      "Episode 39000 Average Reward: -198.17\n",
      "Episode 39100 Average Reward: -199.58\n",
      "Episode 39200 Average Reward: -198.89\n",
      "Episode 39300 Average Reward: -198.0\n",
      "Episode 39400 Average Reward: -195.09\n",
      "Episode 39500 Average Reward: -198.63\n",
      "Episode 39600 Average Reward: -194.63\n",
      "Episode 39700 Average Reward: -199.41\n",
      "Episode 39800 Average Reward: -198.86\n",
      "Episode 39900 Average Reward: -194.89\n",
      "Episode 40000 Average Reward: -197.72\n",
      "Episode 40100 Average Reward: -194.39\n",
      "Episode 40200 Average Reward: -194.45\n",
      "Episode 40300 Average Reward: -200.0\n",
      "Episode 40400 Average Reward: -200.0\n",
      "Episode 40500 Average Reward: -193.61\n",
      "Episode 40600 Average Reward: -193.54\n",
      "Episode 40700 Average Reward: -200.0\n",
      "Episode 40800 Average Reward: -194.54\n",
      "Episode 40900 Average Reward: -197.14\n",
      "Episode 41000 Average Reward: -197.93\n",
      "Episode 41100 Average Reward: -192.46\n",
      "Episode 41200 Average Reward: -188.75\n",
      "Episode 41300 Average Reward: -194.7\n",
      "Episode 41400 Average Reward: -196.37\n",
      "Episode 41500 Average Reward: -198.8\n",
      "Episode 41600 Average Reward: -199.08\n",
      "Episode 41700 Average Reward: -197.66\n",
      "Episode 41800 Average Reward: -194.01\n",
      "Episode 41900 Average Reward: -198.44\n",
      "Episode 42000 Average Reward: -198.34\n",
      "Episode 42100 Average Reward: -188.82\n",
      "Episode 42200 Average Reward: -195.23\n",
      "Episode 42300 Average Reward: -192.56\n",
      "Episode 42400 Average Reward: -198.65\n",
      "Episode 42500 Average Reward: -196.43\n",
      "Episode 42600 Average Reward: -189.13\n",
      "Episode 42700 Average Reward: -193.55\n",
      "Episode 42800 Average Reward: -197.84\n",
      "Episode 42900 Average Reward: -192.73\n",
      "Episode 43000 Average Reward: -199.32\n",
      "Episode 43100 Average Reward: -200.0\n",
      "Episode 43200 Average Reward: -198.89\n",
      "Episode 43300 Average Reward: -194.79\n",
      "Episode 43400 Average Reward: -193.33\n",
      "Episode 43500 Average Reward: -190.99\n",
      "Episode 43600 Average Reward: -189.31\n",
      "Episode 43700 Average Reward: -190.6\n",
      "Episode 43800 Average Reward: -194.23\n",
      "Episode 43900 Average Reward: -197.04\n",
      "Episode 44000 Average Reward: -192.45\n",
      "Episode 44100 Average Reward: -187.78\n",
      "Episode 44200 Average Reward: -175.21\n",
      "Episode 44300 Average Reward: -197.77\n",
      "Episode 44400 Average Reward: -187.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 44500 Average Reward: -196.44\n",
      "Episode 44600 Average Reward: -191.78\n",
      "Episode 44700 Average Reward: -188.49\n",
      "Episode 44800 Average Reward: -192.33\n",
      "Episode 44900 Average Reward: -197.9\n",
      "Episode 45000 Average Reward: -185.89\n",
      "Episode 45100 Average Reward: -188.11\n",
      "Episode 45200 Average Reward: -182.64\n",
      "Episode 45300 Average Reward: -188.0\n",
      "Episode 45400 Average Reward: -189.09\n",
      "Episode 45500 Average Reward: -187.65\n",
      "Episode 45600 Average Reward: -186.13\n",
      "Episode 45700 Average Reward: -176.12\n",
      "Episode 45800 Average Reward: -178.01\n",
      "Episode 45900 Average Reward: -181.24\n",
      "Episode 46000 Average Reward: -197.8\n",
      "Episode 46100 Average Reward: -197.1\n",
      "Episode 46200 Average Reward: -198.72\n",
      "Episode 46300 Average Reward: -195.03\n",
      "Episode 46400 Average Reward: -192.18\n",
      "Episode 46500 Average Reward: -189.61\n",
      "Episode 46600 Average Reward: -187.91\n",
      "Episode 46700 Average Reward: -178.99\n",
      "Episode 46800 Average Reward: -180.89\n",
      "Episode 46900 Average Reward: -193.71\n",
      "Episode 47000 Average Reward: -191.15\n",
      "Episode 47100 Average Reward: -188.6\n",
      "Episode 47200 Average Reward: -195.73\n",
      "Episode 47300 Average Reward: -193.47\n",
      "Episode 47400 Average Reward: -188.91\n",
      "Episode 47500 Average Reward: -182.01\n",
      "Episode 47600 Average Reward: -192.55\n",
      "Episode 47700 Average Reward: -186.97\n",
      "Episode 47800 Average Reward: -198.77\n",
      "Episode 47900 Average Reward: -196.9\n",
      "Episode 48000 Average Reward: -182.24\n",
      "Episode 48100 Average Reward: -174.77\n",
      "Episode 48200 Average Reward: -196.22\n",
      "Episode 48300 Average Reward: -178.27\n",
      "Episode 48400 Average Reward: -188.93\n",
      "Episode 48500 Average Reward: -196.97\n",
      "Episode 48600 Average Reward: -183.24\n",
      "Episode 48700 Average Reward: -160.71\n",
      "Episode 48800 Average Reward: -192.41\n",
      "Episode 48900 Average Reward: -178.81\n",
      "Episode 49000 Average Reward: -188.4\n",
      "Episode 49100 Average Reward: -179.39\n",
      "Episode 49200 Average Reward: -182.4\n",
      "Episode 49300 Average Reward: -194.01\n",
      "Episode 49400 Average Reward: -188.92\n",
      "Episode 49500 Average Reward: -186.15\n",
      "Episode 49600 Average Reward: -183.45\n",
      "Episode 49700 Average Reward: -182.96\n",
      "Episode 49800 Average Reward: -174.68\n",
      "Episode 49900 Average Reward: -174.27\n",
      "Episode 50000 Average Reward: -161.41\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "# Import and initialize Mountain Car Environment\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "# Define Q-learning function\n",
    "def QLearning(env, learning, discount, epsilon, min_eps, episodes):\n",
    "    # Determine size of discretized state space\n",
    "    num_states = (env.observation_space.high - env.observation_space.low)*\\\n",
    "    np.array([10, 100])\n",
    "    num_states = np.round(num_states, 0).astype(int) + 1\n",
    "    # Initialize Q table\n",
    "    Q = np.random.uniform(low = -1, high = 1,\n",
    "    size = (num_states[0], num_states[1],\n",
    "    env.action_space.n))\n",
    "    # Initialize variables to track rewards\n",
    "    reward_list = []\n",
    "    ave_reward_list = []\n",
    "    # Calculate episodic reduction in epsilon\n",
    "    reduction = (epsilon - min_eps)/episodes\n",
    "    # Run Q learning algorithm\n",
    "    for i in range(episodes):\n",
    "        # Initialize parameters\n",
    "        done = False\n",
    "        tot_reward, reward = 0,0\n",
    "        state = env.reset()\n",
    "        # Discretize state\n",
    "        state_adj = (state - env.observation_space.low)*np.array([10, 100])\n",
    "        state_adj = np.round(state_adj, 0).astype(int)\n",
    "        while done != True:\n",
    "            # Render environment for last five episodes\n",
    "            if i >= (episodes - 20):\n",
    "                env.render()\n",
    "            # Determine next action - epsilon greedy strategy\n",
    "            if np.random.random() < 1 - epsilon:\n",
    "                action = np.argmax(Q[state_adj[0], state_adj[1]])\n",
    "            else:\n",
    "                action = np.random.randint(0, env.action_space.n)\n",
    "            # Get next state and reward\n",
    "            state2, reward, done, info = env.step(action)\n",
    "            # Discretize state2\n",
    "            state2_adj = (state2 - env.observation_space.low)*np.array([10, 100])\n",
    "            state2_adj = np.round(state2_adj, 0).astype(int)\n",
    "            # Allow for terminal states\n",
    "            if done and state2[0] >= 0.5:\n",
    "                Q[state_adj[0], state_adj[1], action] = reward\n",
    "\n",
    "            # Adjust Q value for current state\n",
    "            else:\n",
    "                delta = learning*(reward + discount*np.max(Q[state2_adj[0], state2_adj[1]]) - Q[state_adj[0], state_adj[1],action])\n",
    "                Q[state_adj[0], state_adj[1],action] += delta\n",
    "            # Update variables\n",
    "            tot_reward += reward\n",
    "            state_adj = state2_adj\n",
    "        # Decay epsilon\n",
    "        if epsilon > min_eps:\n",
    "            epsilon -= reduction\n",
    "        # Track rewards\n",
    "        reward_list.append(tot_reward)\n",
    "        if (i+1) % 100 == 0:\n",
    "            ave_reward = np.mean(reward_list)\n",
    "            ave_reward_list.append(ave_reward)\n",
    "            reward_list = []\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Episode {} Average Reward: {}'.format(i+1, ave_reward))\n",
    "    env.close()\n",
    "    return ave_reward_list\n",
    "# Run Q-learning algorithm\n",
    "rewards = QLearning(env, 0.2, 0.9, 0.8, 0, 3000)\n",
    "# Plot Rewards\n",
    "plt.plot(100*(np.arange(len(rewards)) + 1), rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward vs Episodes')\n",
    "plt.savefig('rewards.jpg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748bbc9",
   "metadata": {},
   "source": [
    "##https://gist.github.com/ByungSunBae/563a0d554fa4657a5adefb1a9c985626?fbclid=IwAR0yhyOPrM5MelennwCMw1A2Tm867tvkCk0ioud_u8TR8Fq6A-aY4F_vfgk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d048aab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
